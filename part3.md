# Разработка веб-сервисов на Golang (Go). Часть 3

[Go course, MRG, Романов Василий](README.md)
[Разработка веб-сервисов на Golang (Go), Василий Романов, stepik](https://stepik.org/187490)

Неделя 9 - архитектура приложения (Структура приложения)
- 9.1 Структурируем приложение 
- 9.2 Тестируем комплексное приложение 
- 9.3 Авторизация и пароли 
- 9.4 CSRF-токены 
- 9.5 Сессии 
- 9.6 Задание 9 - архитектура типового приложения

Неделя 10 - oauth и рефакториг приложения (Проектирование API)
- 10.1 OAuth 
- 10.2 Немного рефакторинга 
- 10.3 Проектирование API 
- 10.4 Задание 10 - телеграм бот

Неделя 11 - GraphQL
- 11.1 Основы GraphQL 
- 11.2 GraphQL - интеграция в проект 
- 11.3 Организация пакетов в приложении 
- 11.4 Задание 11 - маркетплейс на основе GraphQL

Неделя 12 - сборка, docker, S3 storage, трейсинг
- 12.1 Сборка docker-контейнера 
- 12.2 Хранение файлов в проекте через S3 
- 12.3 Конфигурирование приложения 
- 12.4 Трейсинг запросов 
- 12.5 Обратная связь 
- 12.6 Задание 12 - многопользовательская MUD на основе асинхрона

## part 3, week 1 (09)

Структура приложения
[Код, домашки, литература](week_09/materials.zip)

UPD:
- `handouts/golang_web_services_2023-12-28.zip/9/`
- `handouts/golang_web_services_2024-04-26.zip/9/`

```s
local module=week09
pushd ${PRJ_DIR}/sandbox
mkdir -p ${module} && pushd ./${module}
go mod init ${module}
cat > main.go << EOT
package main
func main() { panic("not yet") }
EOT
go mod tidy
popd # sandbox
go work use ./${module}        
gofmt -w ${module}
go test -v ${module}
go run ${module}
```
[week 9 playground](./sandbox/week09/main.go) `GO_APP_SELECTOR=week09 gr`

### Структурируем приложение - 1 (вводная)

На примере цельного приложения `photolist` рассмотрим несколько важных тем (задачи/проблемы в веб-разработке).
Аппа: авторизация, картинки (лента), комментарии, последователи (подписка на юзеров), рекомендованные юзеры, etc.

### Структурируем приложение - 2 (описание прототипа и его проблем)

Начнем с прототипа, сляпанного из спагетти-кода.
Чтобы сразу нарисовать целостную картину и декларировать проблемы, решаемые в следующих лекциях.
`handouts\golang_web_services_2023-12-28.zip\9\photolist\0_basic\`

- [main](week_09/basic_main.go)
- [photos](week_09/basic_photos.go)

Главная программа: веб-сервис и модуль с утилитами для фоток.
Фунциональность: одна страница, на которой мы можем выбрать фотку для upload и видеть превьюхи уже загруженных фоток.

Три хандлера: `/`: list, `/upload`: upload, `/images/`: static_files

Проблемы:
- глобальные переменные, не-потоко-безопасные;
- постоянный парсинг шаблона;
- нет авторизации;
- файлы на диске сервера (вместо абстрактного хранилища);
- обработка файлов (для каждой превью) прямо при обработке запроса (создание превьюшек должно быть через очередь, в воркерах);
- код плохо разбит на модули, сложно тестировать;
- статика отдается не nginx а самой аппой.

Далее: разделим на компоненты.

### Структурируем приложение - 3 (распил на модули)

Разделение приложения на модули/слои (файлы go).
На части разделился main: handlers, storage, templates.
`handouts\golang_web_services_2023-12-28.zip\9\photolist\1_storage\`

- [main](week_09/storage_main.go)
- [handlers](week_09/storage_handlers.go)
- [storage](week_09/storage_storage.go)
- [templates](week_09/storage_templates.go)
- [photos](week_09/storage_photos.go)

При старте программы создается структура PhotolistHandler с инициализацией полей Storage, Templates (через фабрики).
Хендлеры страниц реализованы как методы этой структуры и пользуются этими полями, отвечающими некоторому нашему интерфейсу.

Шаблон теперь парсится один раз, при старте программы. Уже лучше.

Работа с файлами через Storage, работа с шаблонами через Template.
Функциональность немного абстрагирована, разбита на модули и может быть протестирована.

### Структурируем приложение - 4 (add DB)

Добавим БД `mysql` для поддержки постоянного хранения метаданных (списка загруженных файлов).
`handouts\golang_web_services_2023-12-28.zip\9\photolist\2_tests\`

- [main](week_09/db_main.go)
- [db_init.sql](week_09/db_db_init.sql)
- [storage](week_09/db_storage.go)

При создании структуры с данными сервиса мы кладем туда реф. на хранилище с тем-же интерфейсом но использующее mysql.

Обсудили еще раз проблему глобальных переменных и их инициализации внутри фунции, 
e.g. с помощью конструкции `db := createConnect(...)`: тут не инициализируется глобальная переменная,
тут создается новая локальная переменная.

База крутится в докере, поэтому: `pushd sandbox/week09/ && docker compose up&`

Запустил аппу, загрузил картинки, перезапустил аппу: картинки по прежнему в ленте.
ОК, работает как ожидалось. Есть persistence.

### Тестируем комплексное приложение - 1 (sql-mock)

Как тестировать логику, завязанную на запросы к БД (Storage).
Использовать драйвер к `sql.DB`: go-sqlmock.
`handouts\golang_web_services_2023-12-28.zip\9\photolist\2_tests\storage_test.go`

- [storage_test](week_09/db_storage_test.go)

Мокать БД и ее методы (методы интерфейса sql.DB)
```hs
db, mock, err := sqlmock.New()
st := NewDbStorage(db)
mock.ExpectExec(`INSERT INTO photos`).WithArgs(userID, path).WillReturnResult(sqlmock.NewResult(1, 1))

pushd sandbox/week09/2_sql_storage_test/
go mod tidy
go test -v .
=== RUN   TestStorageAdd
--- PASS: TestStorageAdd (0.00s)
=== RUN   TestStorageGetPhotos
--- PASS: TestStorageGetPhotos (0.00s)
PASS
```
snippets

https://github.com/DATA-DOG/go-sqlmock
> the driver allows to mock any sql driver method behavior.

Через мок мы конструируем ожидаемое поведение слоя бд, это позволяет нам сделать 100% test coverage.
Но это не позволяет нам проверить реальное поведение реальной бд.

Ну, как бы, не совсем то тестируем. На добавлении картинки надо проверить, что в бд появилась запись с требуемыми значениями.
На выборке: что вынули из бд то, что в бд лежит.

Не забываем про интеграционные и e2e тесты.

https://t.me/c/1207533184/57923
```
Про тестирование "хранилища" photolist, с помощью sqlmock
https://stepik.org/lesson/1133614/step/1?unit=1145238
Не смог удержаться, сорян:

Подозреваю, что в лекции не хватает нескольких слов о потенциальном вреде такого мокания.

Через мок мы конструируем ожидаемое поведение слоя бд, это позволяет нам сделать 100% test coverage.
Но это не позволяет нам проверить реальное поведение реальной бд.
Мы можем навалять нерабочие (в реале) запросы и при этом получить зеленые тесты 100% покрытия.
Разраб пойдет спать с чистой совестью а прод отвалится.

Ну, как бы, не совсем то тестируем. На добавлении картинки надо проверить, что в бд появилась запись с требуемыми значениями.
На выборке: что вынули из бд то, что в бд лежит.
Поэтому: не забываем про интеграционные и e2e тесты.
```

### Тестируем комплексное приложение - 2 (gomock)

Как тестировать хендлер веб-страницы на примере `List`.
`handouts\golang_web_services_2023-12-28.zip\9\photolist\2_tests\handler_test.go`,
`handlers_mock.go`

- [handlers](week_09/test_handlers.go)
- [handler_test](week_09/test_handler_test.go)

Сначала посчитали, сколько и каких кейсов тестирования надо: получение фоток (ok, err), заполнение шаблона (ok, err).
Итого 4 кейса.
Сообразили, что нужен mock Storage.

Поэтому: в структуре данных сервиса реф-хранилища меняем на реф-интерфейса.
Потом в тесте подкладываем мок хранилища в код сервера и тестируем разные случаи.

Мок для интерфейса хранилища:
https://github.com/uber-go/mock

По интерфейсу кодогенератор сделает обвязку, 
`mockgen -source=handlers.go -destination=handlers_mock.go -package=main Storage`
В тестах используем её `ctrl := gomock.NewController(t)`

Upload handler будем мокать и тестировать потом, когда абстрагируем хранилище и задействуем облако.

NB: интерфейс должен объявляться в том месте, где он будет использоваться (где наиболее общий код).
Кроме случаев с интерфейсами общего назначения.
Почему? Направление зависимости должно быть в обратную сторону (потребитель зависит от сервиса).
Реализация не должна объявлять этот интерфейс, просто реализовать заявленные методы.
Реализация никогда не содержит объявление интерфейса, во избежание циклический зависимостей.
Напоминает type-class подход.

```
pushd sandbox/week09/2_sql_handlers_test/
go mod tidy
go test -v .
=== RUN   TestHandlerGetPhotos
--- PASS: TestHandlerGetPhotos (0.00s)
PASS
```

### Авторизация и пароли - 1 (auth MVP)

Базовая версия подсистемы идентификации-авторизации-сессий.
`handouts\golang_web_services_2023-12-28.zip\9\photolist\3_auth\`

- [db_init.sql](week_09/auth_db_init.sql)
- [templates](week_09/auth_templates.go)
- [main](week_09/auth_main.go)
- [session](week_09/auth_session.go)
- [index](week_09/auth_index.go)
- [user](week_09/auth_user.go)

Таблица с сессиями, таблица с пользователями.
Для просмотра БД использует веб-морду Adminer.
Регистрация нового пользователя, выдача ему сессии-куки.

В отдельной таблице sessions маппинг sessionID:userID (один пользователь может иметь много сессий).
Таблица пользователей users(id, login, password).

Появились шаблоны страниц регистрации и "авторизации".

Появилась структура UserHandler(db, templates) для обеспечения веб-хендлеров по юзеру.
Появились урлы (и хендлеры) login, logout, reg.
Завернул все урлы в AuthMiddleware через http.Handle `/` (через заворачивание обработчиков в мультиплексор mux.HandleFunc).

Потом добавил статику `/images/`, получилось, что картинки отдаются мимо авторизации, ибо при маршрутизации сначала проверится
самый длинный путь (images) и дернется его обработчик, без авторизации.

http.Handle получает запросы и картинки направляет (момо авторизации) в статику, остальное - в мультиплексор через авторизацию.

Мидлварь авторизации проверяет, надо ли требовать наличия пользователя на данной странице.
На некоторых страницах пользователя может и не быть (e.g. регистрация нового пользователя).

Мапка, где значения это пустые структуры `struct{}{}` (aka unit), они не требует памяти под значения. Это set ключей фактически.
Далее мидлварь достает сессию из БД или создает новую или вываливается с ошибкой.
Сессия это два значения (sessionID, userID). sessionID мы читаем/пишем в куку.

Сессия добавляется в контекст и дергается downstream обработчик запроса, с передачей ему контекста.
Тут меседж про то, что концептуально request.контекст хорошо подходит для помещения данных сессии в него (метаданные запроса).

Проверка наличия сессии в контексте используется в главной странице,
которая решает куда направить пользователя: на страницу логина или в его ленту фоток.

Обработка логина/регистрации, через БД: логин через post (если не post, то вывести форму логина).
Регистрация аналогично.

Тонкости проверки/хранения пароля (с солью) в след.видео.

### Авторизация и пароли - 2 (password hash)

Как хешировать и хранить пароль.
`handouts\golang_web_services_2023-12-28.zip\9\password_hashing\`

- [user](week_09/auth_user.go)
- [pass](week_09/hashing_pass.go)

`pushd sandbox && go test -bench . -benchmem week09`

Рандомная соль + функция хеширования = для одинаковых паролей получаем разные хеши.
Argon2 хорошая функция хеширования.

https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Password_Storage_Cheat_Sheet.md

`hashedPass := argon2.IDKey([]byte(plainPassword), []byte(salt), 1, 64*1024, 4, 32)`

Пример с разными реализациями хеширования пароля.
Бенчмарк по скорости генерации пароля: чем медленнее, тем лучше, затрудняет брут-форс атаки.
Чем больше жрет памяти, тем лучше, труднее запустить на акселераторе.

### CSRF-токены (JWT для защиты от CSRF)

# I_AM_HERE

- [hash_token](week_09/csrf_token_hash_token.go)
- [crypt_token](week_09/csrf_token_crypt_token.go)
- [jwt_token](week_09/csrf_token_jwt_token.go)

JSON Web Tokens: безопасная передача токенов кросс-сайт. JWT.IO для проверки и де/кодирования JWT.
JWT в качестве CSRF-токенов.
Данные в токене открыты и доступны для просмотра. Защищены от изменения.

CSRF: Cross-Site Request Forgery.

Хеш токен просто хеширует массив байт, в котором зашиты sessID, userID. Потом туда добавляется tokenExpTime.

В крипто токене: структура с тремя полями превращается в json и этот текст шифруется.
Требуется защить эти данные от подделок (не от кражи).

JWT предлагает для защиты: контейнер с подписью. Данные открыты, но для их подмены надо как-то узнать ключ, которым подписан токен.
Этот ключ создается и используется на сервере, должен быть одноразовым и защищенным.

JWT токен состит из трех частей:
- заголовок: алгоритм, тип токена
- данные: json с нашими айдишками и временем жизни и временем выдачи токена
- подпись

Игрушечный пример со скользящими ключами, меняемыми на основе времени выдачи/устаревания.

### Сессии - 1 (рефакторинг сессий)

- [main](week_09/jwt_sess_main.go)
- [session_common](week_09/jwt_sess_session_common.go)
- [session_db](week_09/jwt_sess_session_db.go)
- [user](week_09/jwt_sess_user.go)

Рефакторинг кода сессий, с целью абстрагироваться от реализации.
С тем, чтобы потом воткнуть туда JWT токены.

Добавил менеджер сессий, класс с фабрикой.
Менеджер сессий добавлен как поле в структуру хендлеров страниц.
Используется в мидлварь авторизации и в обработке юзера.

Интерфейс менеджера сессий.

Реализация менеджера сессий на БД. Удаление всех сессий пользователя.

Создание пользователя и логин. Смена пароля пользователя.
Добавление атрибута `ver` к данным пользователя. Версионированность.

### Сессии - 2 (stateful vs stateless sessions)

Проблемы JWT в роли замены sessionID.

Аутентификация, stateful сессия, хранится в БД.
Недостаток: надо хранить в БД.
Преимущества: удобно, всё под рукой, любую сессию можно закрыть в любой момент.

Stateless сессия, на токенах (JWT).
В токене хранится не сессия а сразу (UserID, UserRoles).
В БД хранится только инфа по пользователю, сессия не нужна.

Проблемы.
Трудно отозвать сессию, только перевыпустив токен с датой устаревания в прошлом.
При этом умрут все сесии.
Нет надежного logout, при смене пароля не могу закрыть устаревшие сессии.
Нет полного списка сессий, нельзя видеть, кто еще (где) ходит от моего пользователя.

В целом, при отказе от сессий, либо трудно/невозможно выкинуть злоумышленника, либо пользователя постоянно выкидывает и ему надо перелогиниваться.

Как вариант, хранить в БД профиль пользователя а в токене передавать только (userID, userVersion).
Тогда, в профиле можно держать версию данных пользователя и испльзовать ее как признак смены сессии.
Такое, половинчатое решение, все равно ходим в БД за профилем (вместо хождения за сессией); все равно нет данных по каждой сессии.

https://www.google.com/search?q=stateless+sessions+JWT+problems+&client=safari&rls=en&ei=y0YgY56MGvCvrgSS6KewBA&ved=0ahUKEwjenfaBtJH6AhXwl4sKHRL0CUYQ4dUDCA0&uact=5&oq=stateless+sessions+JWT+problems+&gs_lcp=Cgdnd3Mtd2l6EAMyBQghEKABOgoIABBHENYEELADOgUIABCiBDoHCAAQHhCiBEoECEEYAEoECEYYAFCfDVjkOmCgPmgDcAF4AIABVIgB0wWSAQIxMZgBAKABAcgBCMABAQ&sclient=gws-wiz

### Сессии - 3 (JWT session)

- [main](week_09/jwt_sess_main.go)
- [session_jwt](week_09/jwt_sess_session_jwt.go)
- [session_jwt_ver](week_09/jwt_sess_session_jwt_ver.go)

Реализация сессий на JWT в двух вариантах: беззащитном, с версионированием профиля пользователя.
Начинается с менеджера сессий: `sm := NewSessionsJWT("golangcourseSessionSecret")` vs `sm := NewSessionsJWTVer("golangcourseSessionSecret", db)`

Сессия без базы экономит один поход в БД на каждый запрос.
Токен содержит только userID и стандартные поля.
Сессия пишется только в куку.
На удалении сессий мы можем только поставить устаревание куки и ничего не можем сделать с другими сессиями.

N.B. в реализации с сессиями в БД, при удалении сесии дропались записи сессий в БД. Что выкидывало пользователей на других сессиях.
Что хорошо для безопасности.
Без БД эта функциональность недоступна.

Что продемонстрировано сменой пароля к сайту в одном браузере и продолжающей работать сессией в другом браузере.

Реализация сессий на токенах (куках) с версионированием профиля пользователя.
Есть поход в БД за профилем, на каждый запрос.
В токен добавлено поле `ver`.
При проверке сессии нужен поход в БД за профилем пользователя.
После чего проверяется версия, наряду с другими проверками.

При удалении сессий логика не поменялась, удаляется только кука с токеном (userID).
Т.е. выкидывать других пользователей будет только при редактировании профиля (изменении пароля, etc.)

Вопрос: ну и зачем мы так стремимся отказаться от хранилища сессий?
Рекомендует не отказываться от stateful (persistent) session до тех пор, пока выбора не останется.

### week9 homework
???

## part 3, week 2 (10)

Проектирование API
[Код, домашки, литература](week_10/materials.zip) https://cloud.mail.ru/public/PME6/Mqp8mFsPK

### Oauth - 1 (демонстрация получения данных от провайдера oauth)

- [main](week_10/oauth_main.go)

Как реализовать в своем приложении аутентификацию при помощи протокола OAuth.
Демонстрация получения данных пользователя от провайдера OAuth.
Пользователю дают возможность использовать провайдера OAuth (сайт на котором у п. есть учётка) для
выдачи своих данных (userID, ...) сайту photolist.

Надо зарегать свое приложение на специальной странице VK, получив идентификатор и секретный ключ.
При настройке можно указать, к каким данным пользователя сможет обращаться приложение: друзья, фоточки, ...

При нажатии на кнопку "авторизоваться" в приложении, идет обращение к VK с запросом подтверждения "вы уверены ...?".
После подтверждения, идет редирект на страницу приложения, где демонстрируется успешность доступа к данным пользователя VK.
Страница приложения получила от VK код, по которому, дернув API VK, приложение получает токены и данные пользователя.
Теперь приложение может создать сессию для пользователя.

N.B. провайдер спрашивает у пользователя подтверждение только один раз. Потом аутентификация будет происходить прозрачно.

Пакеты `oauth2`, `oauth2/vk`.
В константах зашиты идентификатор приложения, ключ доступа к VK, урл для получения токена, урл для получения данных о пользователе.

### Oauth - 2 (photolist oauth)

- [user](week_10/photolist_oauth_user.go)

Как в игрушечном приложении был добавлен метод аутентификации oauth.

Порефакторил метод регистрации, вынес работу с БД в отдельный метод.
При регистрации нового пользователя, первым делом проверяется наличие такого логина.
Если есть, возвращается его id и ошибка errUserExists.
Это работает для регистрации через VK.
Неважно, как был зареган пользователь, в БД надо внести запись.

Пользователь в VK может не иметь почты, поэтому есть код проверки и использования vkid вместо почты, в качестве логина.
Пароль генерит случайный, для записи в БД. Поэтому войти в приложение с паролем, мимо VK, не получится.

### Немного рефакторинга - 1 (photolist frontend)

- [list.html](week_10/photolist_frontend_list.html)
- [list.js](week_10/photolist_frontend_list.js)
- [templates](week_10/photolist_frontend_templates.go)
- [handlers](week_10/photolist_frontend_handlers.go)
- [storage](week_10/photolist_frontend_storage.go)

Приукрасил страницы аппы, используя bootsrap. Добавил CSRF токены.
Появилась директория `static`, `templates`.

В метатеге html появился `csrf-token`. Который прокидывается в заголовки запроса.

Смотри `rateCommentToggle` для понимания работы лайков.

В бд новая таблица user_photo_likes(photo_id, user_id).
При выводе фоток делает left_join фоток с лайками, с фильтром по пользователю.
Обработка null при выборе лайкнутых фоток.

Обработка записи лайка в БД, handler `Rate`.
Добавить или удалить запись в кросс-таблицу user_photos_likes и обновить количество лайков в таблице photos.
Хотя рейтинг можно считать на лету из кросс-таблицы, экономнее хранить агрегат в таблице фоток.

hot reload: перезапуск сервиса при изменении кода ...

### Немного рефакторинга - 2 (photolist frontend, air)

- [air.conf](week_10/photolist_frontend_air.conf)
- [main](week_10/photolist_frontend_main.go)
- [templates](week_10/photolist_frontend_templates.go)

live reload, hot reload.

`go run --tags=dev .`

`cosmtrek/air` - live-reloading wrapper.

`air -c air.conf`

Поменял подключение шаблонов, стал использовать `vfstemplate` из `shurcool/httpfs/html/`.

### Немного рефакторинга - 3 (static assets)

- [templates](week_10/photolist_frontend_templates.go)
- [assets](week_10/photolist_frontend_assets.go)
- [assets_gen](week_10/photolist_frontend_assets_gen.go)
- [main](week_10/photolist_frontend_main.go)

Встраивание статики в наш бинарник, чтобы фавиконки и прочее отдавать из бинарника а не из внешних файлов.
Проблема: air запускает бинарник из временного каталога, в котором нет нашего хранилища стат. файлов.
Решение: либо абсолютный путь к хранилищу, либо паковать внешние файлы вместе с бинарником.

Resources embedding.

На диске лежит два бинарника: dev, release. dev собирается без embedding, release собирается с embedding.
Ибо внутренние ресурсы программы лучше паковать внутрь программы (до некоторых пор).

Пакуются они через превращение в файлы go (`assets_vfsdata.go`), где ресурсы это слайсы байт.
Очевидно, такое решение годится только для маленьких ресурсов.

В файле ресурсов есть тег `// +build !dev`
чтобы сборка dev пользовалась внешними ресурсами.

В файле ассетов есть тег `// +build dev`. Так определяется, какой файл вливать в бинарник при сборке разных версий.

Генерация ресурсов выполняется очень запутанной процедурой, с использованием тегов `// +build ignore`, `// +build !dev`
в разных файлах.

Запускается генерация файла ресурсов через `go generate --tags=dev`

Не надо встраивать конфиги в бинарник (спасибо кэп).

Не надо встраивать шаблоны страниц сайта в бинарь, если разработка фронта отделена от разработки бэка.
Надо сделать hot-reload шаблонов в аппе, считывая их из хранилища.

### Проектирование API - 1 (схема данных, документация)

Веб-API
- Согласованность, все запросы-ответы в едином формате, стиле.
- Расширяемость, нет проблем с добавлением атрибутов, параметров, сущноcтей.
- Документация, должна быть, актуальная, автогенерированная + вручную написанные блоки.

Пример разных форматов:
- Список безымянных значений из разных доменов: очень плохо, нет структуры, нет типизации, нет самодокументируемости.
    Нет возможности удалить значение.
- Список словарей, лучше, но не сильно. Метаданные для ответа в целом вставить некуда, расширение структуры будет выглядеть чужеродно.
- Словарь вложенной структуры. Нелохо. Если разделить на метаданные и данные, будет отлично.

```json
{
    "meta": {},
    "data": {}
}
```

Никогда не отдавайте HTTP-status 200 если была ошибка.
Отдавайте 400 или 500.

Накрайняк, если хотите отделить транспорт от логики,
можно отдать HTTP-status 200 если была ошибка логики, но в ответе должно быть стандартное поле `status` с кодом ошибки.
Логировать эту ошибку надо через доп. заголовок. Т.е. код ошибки логики дублируется в заголовке и в теле ответа.

Документируйте публичные API в коде, генерируйте документацию. Используйте Swagger.
- schema-first
- from comments

Или protobuf + grpc, schema-first подход, когда в proto-файле всё описано.

Или GraphQL.

### Проектирование API - 2 (формат URL)

Какие URL следует использовать (формат), какие -- нет.
От cgi-bin программ, через php модули, через контроллеры запросов, через человеко-читаемые-урл, через REST HTTP verbs,
до `POST /api/v1/jrpc + JSON-RPC` или `POST /api/v1/gql + GraphQL`.

ЧЧУ: `/api/v1/articles/${slug}`.

Как передавать параметры?
- путь = метод, параметры = параметры. Путь должен однозначно определять сущность, для которой надо получить инфу.
- вариант REST: идентификатор зашивается в путь, `/api/v1/photos/${user_id}`. В Go такое сделать сложновато.
    Для реализации удобнее зашивать идентификатор в последнюю часть пути: `photo/comments/${photo_id}`.

Вот так лучше не надо: `photo/${photo_id}/comments/${comment_id}`, теряется понимание и согласованность.

Рекомендует в пути держать имя метода а параметры в query.
Как вариант, основной параметр идет последним элементом пути.

### Проектирование API - 3 (POST vs PUT)

Когда и зачем использовать POST, PUT.

POST: URI identifies the resource that will handle the enclosed entity.
PUT: URI identifies the entity enclosed with the request.

При использовании PUT всегда в пути должен быть идентификатор записи (в БД).
Создание или изменение записи контролируется клиентом.
`PUT /api/v1/photos`, создаст или отредактирует коллекцию фоток.

POST позволяет указать только эндпойнт, валидно: `POST /api/v1/photos`, создаст новую фотку, вероятно.

Для создания фотки удобнее POST, для изменения фотки удобнее PUT.

### Проектирование API - 4 (ajax)

- [handlers](week_10/ajax_handlers.go)
- [main](week_10/ajax_main.go)
- [token_middleware](week_10/ajax_token_middleware.go)
- [user](week_10/ajax_user.go)

Доработка app photolist.

Комментарии к фотографиям, владелец фотки, подписка на пользователя, рекомендованные пользователи.
Лента своих фоток, ленты других пользователей.

Все эти сущности добавлены с целью демонстрации (далее) некоторых деталей разработки.

Проверка CSRF-токен вынесена в мидлварь. Переделана работа с шаблонами.
Шаблон рендерится на параметре типа мапка-строка-интерфейс. Универсальная мапка, в котороую можно положить любые значения.
Метод `Render` структуры обработки шаблонов. Этот метод работает как мидлварь, добавляя в мапку нужные значения, типа csrf-token.

Проверка csrf-token вынесена в `CsrfTokenMiddleware`.
Все запросы на API будут проверять токен, наряду с теми, где еще не выполнена auth (login, reg).
Мимо проверки пролетают обычные запросы GET-не-апи.

Логин и почта это теперь два разных атрибута пользователя.

### Проектирование API - 5 (ajax, пояснения)

- [storage](week_10/ajax_storage.go)
- [httputils](week_10/ajax_httputils.go)
- [list.html](week_10/ajax_list.html)
- [list.js](week_10/ajax_list.js)

Разъяснение, как photolist теперь работает на AJAX (некоторые функции стали называться *API и дергаются скриптом из страниц).

Лента делает три запроса с серверу, чтобы получить страницу с фотками, список фолловеров и список рекомендаций.
Данные заливаются скриптом в div шаблона.

Дополнительный рефакторинг кода, с выносом формирования json ответов в отдельные методы.

Монструозный SQL запрос на получение фоток с комментариями и фолловерами. Надо будет переделывать.

В запросах нет лимитов на количество записей.

Вынос функций в API позволяет поддержать разных клиентов (мобилки, браузеры, ...) с разными требованиями к контенту.
Кому-то нужны комментарии, кому-то не нужны рекомендации, ...

### week10 homework
???

## part 3, week 3 (11)

GraphQL
[Код, домашки, литература](week_11/materials.zip) https://cloud.mail.ru/public/DCe7/owQVvyqYD

### GraphQL - 1 (вводная)

Весь предыдущий рефакторинг (разделение на API и страницы, подгрузка фолловеров и рекомендаций отдельными запросами)
был нужен для демонстрации пользы GraphQL.

Клиенты разные, потребности у них разные (мобилки, браузер, ...). Нужно что-то гибкое, чтобы каждый мог показывать пользователю
только то, что требуется.

Много мелких запросов -- не совместимо с hiload. По каждому запросу надо проверить авторизацию, сессию, ...

Можно для каждого клиента делать свой эндпойнт, дорого и долго.

Поможет GraphQL: язык запросов к API.
- Describe your data: описывается схема данных и связи.
- Ask for what you want: спец.языком написали запрос только на то, что требуется.
- Get predictable results: json с запрошенными данными.

Пример схемы: https://developer.github.com/v4/object/repository/

Playground https://developer.github.com/v4/explorer/

Проблемы: сложность никуда не пропала.
Мы ее убрали из фронтэнда, предоставив инструмен получения нужных данных одним запросом.
Сложность ушла в бэкэнд, где нужно разбирать и выполнять этот запрос. Поддерживать гибкость.

### GraphQL - 2 (photolist graphql schema)

- [schema.graphql](week_11/gqlgen1_schema.graphql)

Как запустить серверную часть GraphQL на Go.

Рассмотрим схему
- В схеме восклицательный знак означает `not null`.
- Квадратные скобки означают `array`.
- Тип данных `ID` представлен строкой.
- Тип `Query` это чтение данных.
- Тип `Mutation` это запись данных.
- Метод (query) `user` требует параметр -- айди пользователя. Как и метод `photos`.
- В комментариях отмечено, как генерировать код по схеме.
- Есть еще тип `Subscription` для веб-сокет канала. Здесь не используется.

На примере плейграунд http://localhost/8080 демонстирует выполнение запросов.

Реализации:
- graphql-go/graphql на рефлексии, руками надо много писать
- 99design/gqlgen на кодогенерации (любимое), по схеме генерит заглушки

### GraphQL - 3 (gqlgen)

- [server](week_11/gqlgen2_server.go)
- [gqlgen.yml](week_11/gqlgen2_gqlgen.yml)
- [models_gen](week_11/gqlgen2_models_gen.go)
- [resolver](week_11/gqlgen2_resolver.go)
- [schema_alt.graphql](week_11/gqlgen2_schema_alt.graphql)

Обзор сгенерированного кода, см `gqlgen1` в материалах.

В каталоге со схемой `go run github.com/99designs/gqlgen init`

Получили сгенерированный по схеме
код, конфиг генератора, гошные структуры данных (модели).
Резолвер (хендлеры запросов) -- генерируется один раз и потом редактируется погромистом.
generated.go -- сам сервер, куча бойлерплейта, парсинг запросов и пр.

Некоторые подробности про конфиг генератора https://gqlgen.com/config
Как задать свои модели, которые уже есть в коде, например, структура пользователя в БД.
Инлайн директивы, которые можно добавить в схему для уточнения -- откуда генератор возьмет инфу и что с ней сделает.

### GraphQL - 4 (gqlgen2, resolver)

- [server](week_11/gqlgen2_server.go)
- [gqlgen.yml](week_11/gqlgen2_gqlgen.yml)
- [photo](week_11/gqlgen2_photo.go)
- [resolver](week_11/gqlgen2_resolver.go)

Добавление логики в сгенерированные в gqlgen1 заглушки.
Объяснение резолверов.

Rsolvers: входные точки обработки запросов.
В сервере добавлены заглушки с тестовыми данными.

В конфиге есть указание на структуру Photo но сказано, что User внутри Photo надо сгенерировать. Почему?
Чтобы сгенерировать код резолвера этой вложенной структуры.
В коде (DB) в Photo нет вложенной структуры User, тогда как в схеме gql есть.
Тут где-то магия превращения ссылки-по-id в записи БД в развернутую структуру, описанную в схеме.
Маппинг моделей базы в модели gql.

Обратите внимание, у модели (структура) есть методы по именам полей (getter).
Видимо, там эта магия (маппинг) и просходит.
Могла бы, если бы у модели была ссылка на БД. На таком, слишком низком, уровне мы не можем получить структуру User по
данным Photo без запроса к БД. Поэтому надо это делать уровнем выше, в резолверах запросов, где есть ссылки на хранилище.

Резолверы дают точки входа в обработку запросов.
Резолверы для чтения, записи, ...
Также резолверы есть на модели;
для резолвера Photo есть метод User для получения того самого вложенного в Photo пользователя (см.конфиг и схему).
Это тот самый сгенерированный резолвер, заказанный в конфиге.
Вот тут тоже маппинг DB => gqlschema работает.

По логу видно, резолвер User был вызван n+1 раз. Проблема: мы не ходим постоянно дергать базу, да еще и n+1 раз.

### GraphQL - 5 (dataloader)

- [server](week_11/gqlgen3_server.go)
- [userloader_gen](week_11/gqlgen3_userloader_gen.go)
- [resolver](week_11/gqlgen3_resolver.go)

Проблема n+1 запросов.
Можно создать огромную нагрузку на бэк.
Решение: батчевать запросы.

Вложенные структуры в схеме. Например, данные пользователя к каждой фотографии.
SQL запрос с джойном двух таблиц, жить можно.
GraphQL дергает сущности по одной.

Проблема известная, решается посредством https://github.com/vektah/dataloaden
кодогенератор загрузчиков данных.
По сути, выполнение серии запросов к примитиву в рамках одного пакетного запроса к хранилищу.

В резолвере, где нужно получить пользователя, вызывается `UserLoader.Load(...)`,
этот метод ждет некоторое время, ждет дополнительных вызовов этого метода, аккумулируюя запросы.
Потому уже весь пакет запросов выполняется в один заход.

Загрузка пользователей сделана через `UserLoaderMiddleware`, ибо нужен доступ к хранилищу, реф на которое в хандлере запросов.
Нужен доступ к айди текущего пользователя (получить фолловеров и рекомендации), через контекст.
Мидлварь создает конфиг лоадера данных но не делает обращений к БД, только готовит шаблоны запросов.
Конкретный обработчик запроса, при необходимости, дернет эти запросы.

Засада в том, что доступ к данным размазывается по абстрациям. Теперь обращение к БД есть мидлвари, не только в хранилище.

Дополнительно: загрузчик данных может работать не только с единичными обьектами, можно грузить списки (батч: слайс слайсов).

### GraphQL - 6 (gqlgen4, complexity)

- [queries.txt](week_11/gqlgen4_queries.txt)
- [generated](week_11/gqlgen4_generated.go)
- [server](week_11/gqlgen4_server.go)

Как можно сделать очень медленнй сервер GraphQL:
запрашивать (рекурсивно) внутрениие структуры, которые на самом деле получаются джойнами таблиц.
Например, для user сразу вместе с профилем пользователя, получить все его photo.
А для этих фото можно запросить данные пользователя, а для этих пользователей запросить фото, ...
Так можно всю БД выбрать в одном запросе.

Надо как-то ограничивать фантазию клиента, иначе он положит сервер.

Решается проблема с помощью параметров `ComplexityRoot..childComplexity`, которые аккумулируют значение сложности в запросах.

Для задания максимально допустимой сложности запроса в `handler.GraphQL` передается `ComplexityLimit(42)`.
Дополнительно, можно переопределить дефолтный способ вычисления сложности для методов.

Ограничения сложности определяются опытным путём, иногда трудно сразу определить, в какую цену встанет та или иная операция.

Всегда ограничивайте сложность, иначе мамкин хакер положит сайт.

### GraphQL - 7 (gqlgen4, params)

- [schema.graphql](week_11/gqlgen4_schema.graphql)
- [queries.txt](week_11/gqlgen4_queries.txt)

Добавляем параметры в запросы данных.

В схеме можно дать докстрингу для элемента, в тройных кавычках.
После чего, в playground можно пользоваться подсказками и вкладкой с генерированной документацией.

Возможность использовать именованные запросы, в которых определены параметры для передачи в селекторы данных.
Значения параметров для таких запросов задаются не в теле запроса а снаружи, в отдельном блоке.

Внутри тела запроса можно ссылаться на значения определенных рядом сущностей.

### GraphQL - 8 (gqlgen5, загрузка файлов)

- [resolver](week_11/gqlgen5_resolver.go)
- [schema.graphql](week_11/gqlgen5_schema.graphql)
- [server](week_11/gqlgen5_server.go)

Как загружать файлы на сервер, GraphQL.

`mutationResolver UploadPhoto`, `scalar Upload`
gqlgen предоставляет тип `Upload` для этого.

Плейграунд не поддерживает загрузку файлов, поэтому демонстрация будет через `curl`.
Демонстрация происходящего при загрузке текстового файлика в демо-хранилище.

Теперь все компоненты есть, для построения сервиса photolist на GraphQL.

### GraphQL - 9 (photolist 100_gqlben)

- [schema.graphql](week_11/100_gqlgen_schema.graphql)
- [user_repo](week_11/100_gqlgen_user_repo.go)
- [user](week_11/100_gqlgen_user.go)
- [main](week_11/100_gqlgen_main.go)

Начал внедрять GraphQL в аппу photolist.

Нужен рефакторинг. В таблице Photo есть (userID, userLogin), что противоречит схеме GraphQL.

Применил шаблон repository, где (`UserRepository`) абстрагировал операции с пользователем.
Изоляция бизнес-логики от логики хранения.
NB: ранее фото и сессия уже использовали этот шаблон, хотя никто и не заметил.

UncleBob: "DB is an IO device!"

В резолверах gqlgen лежат ссылки на репозитории, не на DB.

### GraphQL - 10 (gqlgen6 directive)

- [schema.graphql](week_11/gqlgen6_schema.graphql)
- [generated](week_11/gqlgen6_generated.go)
- [server](week_11/gqlgen6_server.go)

Директивы в GraphQL.
Позволяют сделать типа мидлварь для доступа к данным или аргументам.
Определить дополнительное поведение при обработке неких полей.

Два примера: на обработку результата и на обработку входных параметров.

В сгенерированном коде появился блок `Directives`, с определением функций, вызываемых при применении директив.
В реализации надо будет определить бизнес-логику этих врапперов.

Логика `isSubscribed`: нельзя получить фотки пользователя, на которого слиент не подписан.
Пример того, как можно разграничить доступ по ролям пользователей.

Реализация директив подставляется в конфиг в коде сервера.

Логика директивы `validation` использует параметр: список строк.
Валидация прикручена к комментариям загружаемой фотки `Mutation.uploadPhoto`.
Реализована валидация в методе `CheckValidation`.

### GraphQL - интеграция в проект - 1 (photolist 100_gqlgen backend)

- [schema.graphql](week_11/100_gqlgen_schema.graphql)
- [gqlgen.yml](week_11/100_gqlgen_gqlgen.yml)
- [XXX_graphql_models_gen](week_11/100_gqlgen_XXX_graphql_models_gen.go)
- [XXX_graphql_resolver](week_11/100_gqlgen_XXX_graphql_resolver.go)
- [main](week_11/100_gqlgen_main.go)
- [graphql_middleware](week_11/100_gqlgen_graphql_middleware.go)
- [graphql_resolver](week_11/100_gqlgen_graphql_resolver.go)

Интеграция сервера GraphQL в проект photolist (backend), в endpoint `/graphql`.

Пример сгенерированного с 0 кода.
Стартовая точка (main), подключение резолверов.
UserLoaderMiddleware.
Реализация резолверов.

Демонстрация загрузки фотки через curl.

### GraphQL - интеграция в проект - 2 (photolist 100_gqlgen frontend)

- [handlers](week_11/100_gqlgen_handlers.go)
- [list_gql.html](week_11/100_gqlgen_list_gql.html)
- [list_gql.js](week_11/100_gqlgen_list_gql.js)
- [schema.graphql](week_11/100_gqlgen_schema.graphql)
- [user_repo](week_11/100_gqlgen_user_repo.go)

Демо сайта photolist, где под капотом GraphQL.

Добавил шаблон для загрузки ленты через GQL.
В шаблоне подключается другой JS,

В JS больше всего изменений. See `function getUserPhotos`.
Все данные для рендеринга всей страницы получены одним запросом. Nice.

Обрати внимание на реализацию `function uploadPhoto`. ХитрО сделано , переменная`variables.file` задается через маппинг из поля `my_file`.

Подписанные пользователи и dataloader.
Даталоадер тут не используется, используется join в `LookupByIDs` методе репо.
Структура `User` с полем `Followed`, поле **указатель** на bool, чтобы отличать данные от null.
В итоге, имеем два способа загрузки пользователя из БД (даталоадером и подписками), отличающихся загрузкой этого булева значения.
Сделал херню и подпер костылем: демонстрация того, как надо тщательно следить за схемой, количеством запросов, согласованностью.

next: как разобрать получившуюся кашу из файлов кода в проекте?

### Организация пакетов в приложении - 1 (photolist 101_structure)

- [struct_all_files.txt](week_11/101_structure_struct_all_files.txt)

Сейчас все свалено в кучу. Оно работает, это ок. Если проект маленький, то сложить все модули в один корневой пакет -- норм.

Можно было бы разбить на MVC пакеты, но это не слишком удобно и хорошо. В Go не фанатеют по MVC.

По одному модулю в пакете вообще не надо.

Пакет утилз это свалка, не делайте его. Делайте говорящее имя пакету, разбейте его на специфические пакеты.

See https://github.com/golang-standards/project-layout

Интересное:
- Пакет `internal` защищен компилятором, приватный код проекта.
- Пакет `pkg` это общий код проекта, библиотеки. Можно весь код, кроме main туда положить, при желании.
- Пакет `vendor` для складывания зависимостей, которые не хочется качать при каждой сборке.
- `cmd` для файлов `main.go`, в отличие от `pkg`

Как выглядит photolist в таком раскладе
- Makefile
- Readme.md
- api/schema.graphql
- bin/photolist # собранный бинарь
- cmd/photolist/main.go
- configs/air.conf, gqlgen.yml
- deployments
- pkg
    /assets/assets.go, ...
    /graphql/graphql_generates.go, ...
    /index/index.go
    /photos/handlers.go, ...
    ...
    /utils
        /httputils/httputils.go
        /randutils/randutils.go

Практически весь код проекта лежит в `pkg`.

Проект разбит по сущностям, не по функциям.
DDD, Domain Driven Design. Часто применяется в Go.

При раскладывании файлов префиксы в именах файлов можно убрать, чтобы не получалось `session/session_db.go`

Что насчет импортов и циклических зависимостей?

### Организация пакетов в приложении - 2 (cycle deps)

- [import_cycle.txt](week_11/101_structure_import_cycle.txt)
- [layers_1.txt](week_11/101_structure_layers_1.txt)
- [layers_2.txt](week_11/101_structure_layers_2.txt)

Цикличные зависимости недопустимы.

Автор получил цикл через photos-session-user-session.

Было

`type Session struct` in session_common.go
`type SessionManager interface` держит ссылку на `User`.
Ибо данные сессии это UserID из БД.

`UserHandler` in user.go держит ссылку на `SessionManager`.

Стало

Расшить это можно используя абстрактный интерфейс, от которого зависеть будет нижний слой а верхний будет давать его реализацию.

In session_common.go declare `type UserInterface interface` with methods `GetID(), GetVer()`.
Так сессия уже не зависит от реализации юзера и не импортирует пакет `user`.

Альтернативный подход: отдельный пакет с http_handler, в котором и users и session и photos и index;
модуль юзерс держит реализацию и сесии и юзера.

### Организация пакетов в приложении - 3 (Makefile)

- [Makefile](week_11/101_structure_Makefile)

Рассказал в общих чертах о складыании команд сборки, тестирования, etc в мейкфайл.

`make build`

```s
# виртуальное дерево зависимостей, не реальные файлы а "поддельные" (реальные файлы сломают make или make сломает реальные файлы).
# фони дает билд, билд дает ассеты, ...
.PHONY: buld
build: assets
    go build ...
```

### week11 homework
???

## part 3, week 4 (12)

Сборка docker-контейнера, хранение файлов в S3, трейсинг запросов
[Код, домашки, литература](week_12/materials.zip) https://cloud.mail.ru/public/NteB/XuFz1UTqJ

### Сборка docker-контейнера - 1 (102_build modules)

- [modules.txt](week_12/photolist_102_modules.txt)
- [Makefile](week_12/photolist_102_Makefile)
- [main](week_12/photolist_102_main.go)

Модули в Go 1.11+
Стандарт по управлению зависимостями в проектах.
Разные проекты могут использовать разные версии модулей, модули разложены по версиям и доступны одновременно.

`go mod init photolist` чтобы сделать модуль из проекта.

`GO111MODULE=on go mod ...` для поддержки модулей из GOPATH, на время перехода из до-модульной системы в модульную.
Если без этого, то модули будут грузиться из версионированного кеша модулей в системной папке Go.

Появляется файл `go.mod`.

`make build` вместе с билдом собирает инфу по используемым модулям (latest) и сохраняет ее в `go.mod`, `go.sum`

`go mod download` скачивает все зависимости и кладет их в системный кэш `go/pkg/mod/`

`go mod vendor` складывает зависимости не в системный кэш а в папку `vendor` проекта.
Чтобы работало, собирать проект надо через `go build -mod=vendor ...`

Внутри проекта свои внутренние зависимости импортируются как `photolist/pkg/pkgname`
из модуля photolist.
Если бы инит был сделан как `go mod init github.com/rvasily/photolist`, то импорт бы это отражал:
`github.com/rvasily/photolist/pkg/pkgname` но на гитхаб бы оно не полезло ибо локально уже лежит всё.

Прощай GOPATH.

Транзитивные зависимости?

### Сборка docker-контейнера - 2 (102_build docker)

- [Dockerfile](week_12/photolist_102_Dockerfile)
- [.dockerignore](week_12/photolist_102_.dockerignore)
- [Makefile](week_12/photolist_102_Makefile)
- [Dockerfile.Multistage](week_12/photolist_102_Dockerfile.Multistage)
- [main](week_12/photolist_102_main.go)

Linux CGROUPS, NAMESPACES: изоляция процесса, технологии под капотом докер.

Сборка и запуск в одном контейнере.
Копирование папки с учётом игнорируемых файлов.

`make docker` сборка образа. При сборке обломался на команде `git ...` ибо в образе такого бинаря нет (об этом позже).
Сборка образа заканчивается запуском аппы (нет), ибо последняя команда в Dockerfile это `CMD ...`.
Сборка заканчивается созданием образа.

`make docker_run` запуск контейнера, с пробросом порта tcp. Запускается аппа, через `CMD ...` в Dockerfile.

Размер образов: всё плохо, см. `docker images`.
979 мегабайт на бинарь аппы (образ). Родительский образ, зависимости, исходники, ...

Собираем в одном образе, деплоим в другом образе.
https://github.com/proxeter/go-service-template/blob/master/deployments/docker/Dockerfile
Многошаговая сборка `make docker_multistage` на выходе дает маленький образ, только ОС и аппа.
100 мегабайт.

NB: в программе поменялся коннект к БД, `dsn`. Сеть докера внесла коррективы.
Очевидно, коннект надо пробрасывать как параметр снаружи. Как и другие параметры.

### Сборка docker-контейнера - 3 (docker compose)

- [adminer.dc.yaml](week_12/photolist_102_dev_adminer.dc.yaml)
- [docker-compose.yml](week_12/photolist_102_deploiments_docker-compose.yml)
- [db_init.sql](week_12/photolist_102_dev_db_init.sql)
- [nginx.conf](week_12/photolist_102_configs_nginx.conf)

Docker compose полезная утилита, запуск нескольких контейнеров в связи друг-с-другом.
Более-менее сложные аппы требуют наличия разных сервисов: БД, мониторинг, конфиг, ...
Эти сервисы могут быть в контейнерах и удобно запускать пачку контейнеров одной командой.
Как и конфигурить в одном файле.
Также, compose может использовать swarm, что может быть удобно тем, кто не хочет k8s.

adminer.dc.yaml, иллюстрация простого композа, файл compose для запуска adminer и mysql (из директории dev).
При старте mysql запускается инит базы из db_init.sql (не раскрыта связка docker-entrypoint-initdb.d и db_init.sql).
Устанавливаются переменные среды: пароль и имя БД.
NB пути в конфиге указаны относительно положения yml файла конфига!

docker-compose:
- photolist c предварительной сборкой, линкован по сети с контейнером mysql; зависит от mysql.
    Команда для запуска через скрипт `wait-for-it.sh`, для запуска аппы после появления БД.
- mysql, так-же как и в adminer.dc.yaml;
- nginx просто прокидывает запросы на photolist;
- adminer.

`make docker_compose` -> `docker-compose -f ./deployments/docker-compose.yml up`

### Хранение файлов в S3 - 1 (Simple Storage Service, minio)

- [photolist 102/graphql_resolver](week_12/photolist_102_graphql_resolver.go) UploadPhoto: SaveFile, MakeThumbnails
- [s3/aws](week_12/s3_aws.go)
- [s3/minio](week_12/s3_minio.go)

Как улучшить систему хранения файлов. Локальный хост не годится для масштабирования.
Ничего удобнее S3 пока не придумали.

Программа-демо создания бакета "photolist" и выполнения основных операций с файлами.
`github.com/aws/aws-sdk-go/aws`

При разработке используется локальный сервис - имитации S3, `min.io`.
Проблема MinIO: для успешного восстановления при сбоях надо ReplFactor4 (4 диска). Будьте осторожны!

Есть библиотека `minio/minio-go` если хочется ограничиться именно minio, там есть полезняшки, которых нет в S3.

Есть веб-интерфейс http://127.0.0.1:9000/minio/

### Хранение файлов в S3 - 2 (files direct web access)

- [s3/minio](week_12/s3_minio.go) policy
- [s3/aws policy](week_12/s3_aws.go) policy
- [s3/docker-compose](week_12/s3_docker-compose.yaml) minio
- [s3/configs/nginx.conf](week_12/s3_configs_nginx.conf) images

Как получать доступ через веб. Зачем? Чтобы клиент мог получить файлы напрямую, сразу из хранилища.
Через nginx, чтобы как-то прикрыть хранилище.

Надо открыть доступ к папке для анонимуса.
Делается это через задание политики, `SetBucketPolicy`, `PutBucketPolicy`.

Минио сервис поднимается через композ докера, вместе с nginx.
Там же прописаны ключи ACCESS, SECRET, что плохо (надо их прокидывать из vault), но для дева сойдет.

В nginx прописан прокси на хранилище файлов.

Это было демо на технологии хранения файлов в S3.
Теперь пора все это интегрировать в аппу photolist.

### Хранение файлов в S3 - 3 (photolist S3 integration)

photolist_103/
- [blobstorage/fs](week_12/photolist_103_blobstorage_fs.go)
- [blobstorage/s3](week_12/photolist_103_blobstorage_s3.go)
- [photos/handlers](week_12/photolist_103_photos_handlers.go)
- [photos/utils](week_12/photolist_103_photos_utils.go)
- [graphql/graphql_resolver](week_12/photolist_103_graphql_resolver.go)
- [configs/nginx.conf](week_12/photolist_103_configs_nginx.conf)
- [deploiments/docker-compose.yml](week_12/photolist_103_deploiments_docker-compose.yml)

Интеграция проекта с хранилищем S3.
Пользователю изменения видны только в формате url при просмотре файла.

Новая реализация интерфейса хранилища, `FSStorage`. Даже не новая реализация а новый интерфейс.
Новый пакет `blobstorage` с дефолтной реализацией интерфейса (диск локалхост).

Реализация интерфейса для S3.
Сессия там не из аппы а из S3, NB!
В методе `Put` теперь записываются метаданные файла (user-id).
Нет метода `Read` ибо чтение файлов будет напрямую клиентом через HTTP из хранилища (через nginx).

В хендлерах поменялся код записи файла и MakeThumbnails.
Генерация имени файла, теперь uuid. Потеряли дедупликацию по именам (md5), зато защитились от удаления файла сразу для всех пользователей.
Кардинально поменялся код MakeThumbnails, теперь все на буферах и высокоуровневых абстрактных интерфейсах.

В резолвере UploadPhoto, как и в тамбнейлах, для буфера байт надо сделать обертку Reader, чтобы байты можно было передавать в методы хранилища.
`bytes.NewReaded(buff.Bytes())`

В конфиге прокси изменения связаны с поддержкой user-id в урлах и избыточными заголовками S3.

В композе осталась одна точка входа: порт 8080:80 nginx. Порты аппы наружу не прокидываются.

Аппа стала масштабируемой, можно аппу запускать на нескольких контейнерах.

Далее: откроем тайну user-id в урлах картинок.

### Хранение файлов в S3 - 4 (user-id в урлах картинок)

photolist/104_acl
- [configs/nginx.conf](week_12/photolist_104_configs_nginx.conf)
- [cmd/main](week_12/photolist_104_cmd_main.go)
- [user/user_handlers](week_12/photolist_104_user_handlers.go)

Хотим сделать приватные альбомы, разграничивать доступ по UserID.
Это было бы сделать легко, если бы отдачей картинок занималась аппа.
Но мы сделали отдачу картинок напрямую из хранилища.

nginx спешит на помощь.
Модуль `ngx_http_auth_request_module` https://nginx.org/en/docs/http/ngx_http_auth_request_module.html

Наша аппа дает апи авторизации доступа к картинке, но не обязана качать картинку.

- `auth_request /auth`
- `location = /auth { proxy_pass http://photolist:8080/api ... }`

`http.HandleFunc("/api/v1/internal/images/auth" ...)`

`InternalImagesAuth(...)`

Next: Никто не мешает вынести обработку такой авторизации на отдельный сервис.

### Хранение файлов в S3 - 5 (files auth service)

photolist/104_acl
- [configs/nginx.conf](week_12/photolist_104_configs_nginx.conf)
- [files.txt](week_12/photolist_104_files.txt)
- [Makefile](week_12/photolist_104_Makefile)
- [deployments/docker-compose.yml](week_12/photolist_104_deployments_docker-compose.yml)
- [cmd/photoauth/main](week_12/photolist_104_cmd_photoauth_main.go)

Как вынести авторизацию на доступ к картинкам в отдельный сервис.

`proxy_pass http://photoauth:8080...`

`cmd/photoauth/main.go` Теперь в директории `cmd` лежат две аппы (main пакеты).

`go build ... ./cmd/photoauth`

`photoauth: ... depends_on: ... photolist` NB собирается в том-же образе, что и photolist. Хотя, надо бы в отдельном образе.
Запуск только после сборки и запуска photolist.

Next: теперь пришла пора сделать проброс конфига коннекта к БД, ибо уже два сервиса нуждаются в одной БД.

### Конфигурирование приложения - 1 (viper Config)

photolist/104
- [cmd/photolist/main](week_12/photolist_104_cmd_photolist_main.go)
- [config/cfg ](week_12/photolist_104_config_cfg.go)`type Config struct { ... } ... func Read(...)`
- [configs/photolist.yaml](week_12/photolist_104_configs_photolist.yaml)
- [deployments/docker-compose.yml](week_12/photolist_104_deployments_docker-compose.yml)
- [configs/common.env](week_12/photolist_104_configs_common.env)
- [configs/photoauth.env](week_12/photolist_104_configs_photoauth.env)

Уже два сервиса используют одну БД. Нужно как-то пробрасывать конфиг, задаваемый в одном месте.
Нужна система управления конфигурацией.

Юиблиотека `spf13/viper` https://github.com/spf13/viper
`cfg := &config.Config{} ...`
Демо получение настроект из файла и переменных окружения.

Конфиги будут у нас лежать в yml файлах и переменных окружения.
Все параметры вынесены в конфиг. Никакого хардкода.

Файлы конфигов проброшены через volumes докера, что дает возможность менять конфиги без пересборки контейнера.
Переменные окружения задаются через файлы `env_file: - ../configs/common.env`
https://docs.docker.com/compose/environment-variables/#the-env_file-configuration-option

Особо: у вайпера (на момент съемок видео) есть бага, из-за которой не парсится `example.env2`, поэтому приходится делать
`v1.GetString("example.env2")`

Next: создать еще один микросервис, чтобы получить проблемы и решать их, получая знания.

### Конфигурирование приложения - 2 (отдельный сервис авторизации, grpc)

photolist/104
- [cmd/photoauth/main.go](week_12/photolist_104_cmd_photoauth_main.go)
- [api/auth.proto](week_12/photolist_104_api_auth.proto)
- [go.mod](week_12/photolist_104_go.mod)
- [cmd/auth/main](week_12/photolist_104_cmd_auth_main.go)
- [session/auth](week_12/photolist_104_session_auth.go)
- [session/session_grpc](week_12/photolist_104_session_grpc.go)
- [deployments/docker-compose.yml](week_12/photolist_104_deployments_docker-compose.yml) сервис `auth`

Вынес сервис авторизации, grpc API. Для демонстрации некоторых концепций. В небольших проектах такое решение это явный overkill.

В коде клиента авторизации, создание менеджера сессий `sm, err := session.NewSessionGRPC(...)` реализованного как отдельный сервис.

API сервиса описано в proto файле.
Немного шаманства с переименованием модуля `grpc`.  Импортируется он по одному пути а фактически находится по другому.
Кодогенератор grpc сгенерил заглушки.

Возвращаемый параметр как модель сущности в БД. Для нано-сервиса это ОК, но для сложных случаев надо рассаживать API и модели БД.
В общем случае это разные структуры данных.

Реализация интерфейса менеджера сессий. Тут он рассадил структуры API и реализации, повысив модульность.

Проблемы: слой grpc и слой хендлеров использования сервиса grpc не соответствуют по контексту:
e.g. grpc хочет context (управление таймаутами, etc) а в хендлере его нет.
Интерфейс SessionManager не предусматривает прокидывание контекстов, нужных для выполнения grpc запросов к сервису авторизации.
Архитектура нуждается в рефакторинге, дроблении на слои и добавление интерфейсов в качетсве клея.

Next: как порешать такие проблемы микросервисов.

### Трейсинг запросов - 1 (request_id, trace-id)

Идентификатор запроса, request id, RID, trace_id etc.
Посмотрите заголовки любого HTTP запроса на крупном сайте, типа амазона: `x-amz-rid`.

Идентификатор запроса используется для сквозной идентификации запроса в логах, статистике.
При обработке запроса в ходе его прохождения через разные подсистемы сайта.

Стандартизируется, `trace-context/#trace-id` https://www.w3.org/TR/trace-context/#trace-id

Сделал у себя `X-Request-ID`. Показал как выглядит в логах микросервисов.

### Трейсинг запросов - 2 (параметры запросов graphql в логе)

photolist/105_ctx
- [middleware/accesslog](week_12/photolist_105_middleware_accesslog.go)
- [cmd/photolist/main](week_12/photolist_105_cmd_photolist_main.go)
- [static/js/list_gql.js](week_12/photolist_105_static_js_list_gql.js)
- [graphql/gqlgen_middleware](week_12/photolist_105_graphql_gqlgen_middleware.go)

Как выводить в лог параметры запросов (GraphQL), чтобы в логе было видно операцию, параметры, результаты, время, etc.

Начал с того, что заявил, что хочет отличать операции GraphQL в логах. Сейчас это выглядит просто как запрос на endpoint graphql,
без подробностей.

В миддлваре http handler эти подробности не видны, что делать?
Можно было бы решить на уровне веб-хендлеров, подставляя в урл имя операции `NewRequest(POST, /graphql/ + opName)`.
Или так `NewRequest(POST, /graphql?op= + opName)`
Мы не ищем легких путей.

В теле запроса GraphQL есть поле `operationName`, как оно там появилось?
Это поле приходит из текста запроса js, e.g. `getPhotosQuery` `const getPhotosQuery = query renderUserPage($userID: ID!) ...`
Это имя (renderUserPage) в спеке GraphQL идет как опция, хочу пишу, хочу нет.
Хотя мы имя задали в запросе, он еще раз прописывает его в параметрах `var body = JSON.stringify(params)`.

Получили имя операции в теле запроса. В мидлвари, акеслог, тело недоступно, имя оп. не попадает в акеслог.
Тут все по прежнему.

В RequestContext graphql есть hooks: ResolverMiddleware, RequestMiddleware, Tracer.
- request враппер работает с заголовками, GetRequestContext
- resolver работает с данными, GetResolverContext
- tracer see later

Хуки устанавливаются в main, где цепляются хендлеры запросов.
Смотрим реализацию хуков. Имя операции, заданное в теле запроса в js, доступно в GetRequestContext и попадает в лог
имено в RequestMiddleware.

Есть возможность получать всю инфу в одной строчке accesslog, если мидлварь будет записывать параметры и метрики запроса в контекст,
из которого потом AccessLogMiddleware эту инфу достанет и выведет в лог.

ResolverMiddleware выводит в лог `path` и ошибку резолвера данных.
`path` показывает имя того поля данных, для которого выполняется данный резолвер. Т.е. для структуры с 7 полями будет выполнено 8 резолверов.
И в лог добавится 8 строк.
Такой лог мгновенно засирает память, поэтому надо его включать только на избранные запросы.

Обратите внимание, мы выводим в лог время, затраченное на операции. Это важно для анализа.

### Трейсинг запросов - 3 (Jaeger)

photolist_106_tracing
- [middleware/accesslog](week_12/photolist_106_middleware_accesslog.go)
- [session/common](week_12/photolist_106_session_common.go)
- [graphql/gql_tracer](week_12/photolist_106_graphql_gql_tracer.go)
- [cmd/photolist/main](week_12/photolist_106_cmd_photolist_main.go)

Jaeger: распределенная трассировка.
Измерения (см. пред. лекцию) можно анализировать.

Показыает, как в веб-интерфейсе Jaeger смотреть трассировку.
Поиск трассы по RID: в поле `Tags` веб-формы вбил `myrequestid=$RID_VALUE`, хотя в аппе, в заголовке запроса это
`x-Request-ID`.

По этому RID отображено дерево записей с красивыми графиками. Видно все шаги обработки запроса.

Заострил внимание на том, что 3 мс, которые, как он думал, потрачены на оверхед graphql,
на самом деле потрачены на поход в сервис авторизации (наносервис проверки сессии).

Как это выглядит в коде.

В мидлвари акеслога трейсинг подключается к контексту открытием спана
`opentracing.StartSpanFromContext(...)` который закрывается `defer span.Finish()`.

Там же указывается имя тега `myrequestid` с указанием requestID. Что, в целом избыточно, но демонстрирует работу тегов.

В сессии, в `AuthMiddleware`, мы начинаем с `opentracing.StartSpanFromContext()` и задаем некоторые параметры этому спану.
Закрываем его НЕ через defer а через `span.Finish()` руками, после отработки проверки сессии.
Поскольку внутрь уже не надо пробрасывать текущий спан, контекст не модифицируется.

В обработке трейсинга graphql есть нюанс. Там уже есть поддержка трейсинга, но автор решил, что дефолтная реализация не
подходит для демо, поэтому слегка переписал ее.
В частности, с использованием `OperationName`.
См. `StartOperationExecution`, `EndOperationExecution`.

Как запускается трейсинг? В main, начиная с `jaeger.Configuration{...}`
NB `SamplerConfig`, поддержка отправки части трейсов.
Дополнительно, егерь умеет просылать метрики (собранные вами в аппе) в prometeus, писать логи.

### Трейсинг запросов - 4 (distributed)

photolist/106_tracing
- [session/grcp](week_12/photolist_106_session_grcp.go)
- [cmd/auth/main](week_12/photolist_106_cmd_auth_main.go)

Предыдущий вариант демонстрирует трейсинг в рамках одного процесса (photolist).
Но нас интересует распределенный трейсинг, отслеживание прохождения запроса по разным процессам, хостам.

jaeger умеет в распределенный трейсинг.

RequestID прокидывается через метаданные, см. `func ctxWithTrace(...)`. Это бывший метод `ctxWithRID`.
Чтобы ссылка на родительский спан уехала в другой сервис (grpc), мы используем `opentracing.GlobalTracer().Inject(...)`.
По факту, можно использовать егерьский uber-trace-id в качестве своего RID.
Далее, в каждом методе обработки сессии используется этот модифицированный контекст.

В самом сервисе auth (проверка сессий), есть `func AccessLogInterceptor(...)`.
В нем извлекается клиентский контекст `opentracing.GlobalTracer().Extract(...)`
для создания спана с привязкой к переданному в метаданных родительскому спану `opentracing.StartSpan(...clientContext...)`.

Это демонстрация того, как оно работает. В жизни, скорее всего, вы будете пользоваться библиотеками вроде
`opentracing-contrib/go-grpc` https://github.com/opentracing-contrib/go-grpc

Таким образом, покрывая код спанами, можно всегда найти тормозной участок системы.

### week12 homework
???
