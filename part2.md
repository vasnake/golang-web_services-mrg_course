# Разработка веб-сервисов на Go. Часть 2:

- week 1, Анатомия веб-сервиса
- week 2, SQL NoSQL
- week 3, Микросервисы
- week 4, Сервис в работе

## part 2, week 1 (05)

Анатомия веб-сервиса
[Код, домашки, литература](week_05/materials.zip) https://cloud.mail.ru/public/bjWY/TfQTzVMer

### Приветствие 1

Привет, будем галопом по веб-сервисам. Обзорно. Какие задачи встречаются при разработке веб-сервисов.

### Приветствие 2

Анатомия, компоненты веб-сервиса, которые так-или-иначе существуют в разных фреймворках:
- роутинг: отображение урлов (запросов) на функции
- авторизация/аутентификация/идентификация
- логирование
- обработка ошибок
- валидация входный параметров
- шаблонизация
- конфигурировние
- мониторинг
- миддлваре/фреймворки
- работа с IO device (БД, хранилища, очереди, etc.)

### Middleware 1 (wrappers)

- [middleware](week_05/middleware.go)

- Как мог бы выглядеть код миддлваре, если бы мы всё писали руками. Обработка паники, запись в лог, проверка авторизации, бизнес-логика.
Другой вариант: заворачивание хендлеров в "декораторы" middleware.
Middleware в данном случае это фунция handler => handler. Или ServerMux => ServerMux.
Аспект-ориентированое программирование.

### Middleware 2 (metrics using req.context)

- [context_value](week_05/context_value.go)

Используем Context (запроса) для реализации мидлвари, в которой вычисляются тайминги. Ценный пример мониторинга времени, уходящего на обработку запроса.
`req.Context()` как хранилище данных. Каждый вызов бизнес-логики завернут в декоратор, который, с использованием контекста,
вычисляет время, затраченное операцией. Потом миддлварь отсылает собранные в контексте данные в мониторинг, лог.
Перед выполением запроса контекст инициализируется, создается структура в которую будут собираться данные.
На выход навешивается defer, в котором подбиваются итоги. И вызывается бизнес-логика, в которой контекст будет обновляться при сборе данных мониторинга.

Значения в контексте не типизированные, компайл-тайм проверок нет. Поэтому -- очень осторожно и только если по другому нельзя!

Пустой интерфейс -- зло, реализовывать протоколы через них не надо, проверять и отлаживать очень сложно.
Как вариант -- сделайте свой контекст, с конкретными типами.

### Middleware 3 (errors)

- [basic_err](week_05/basic_err.go)
- [named_err](week_05/named_err.go)
- [own_err](week_05/own_err.go)
- [pkg_err](week_05/pkg_err.go)

Как лучше обрабатывать ошибки (в хендлерах запросов):
собрать и вывести информации побольше; трансформировать ошибку в ошибку с подробностями.

Еще лучше: типизированная ошибка. С помощью `errors.New()` создаются константы-ошибки для использования в коде.

Или создается новый тип данных, структура с расширенным описанием ошибки, для использования в качестве возвращаемого значения.
Достаточно реализовать интерфейс `Error() string` и можно пользоваться структурой как стандартной ошибкой.

Универсальный враппер ошибок сделан в `pkg/errors`. `errors.Wrap(...)`, `errors.Cause(err).(type)`, ...
А еще там есть вывод stacktrace.

### Роутеры 1 (gorilla, httprouter)

- [gorilla](week_05/gorilla.go)
- [httprouter](week_05/httprouter.go)
- [multiple](week_05/multiple.go)

`gorilla/mux`, мощный роутер, позволяет задать метод HTTP (POST, GET, ...), задать сложные правила матчинга не только по урл но и по другим параметрам запроса,
многое другое.
Один из самых медленных роутеров.

`HttpRouter`, вполне годная производительность (prefix-tree). Не такой мощный как горилла, сигнатура хендлеров не совместима с оригинальными гошными хендлерами.

Можно испльзовать разные роутеры в разных частях сервиса. Пример с использованием трех роутеров в одном сервисе.
Tradeoff: speed/power

### Роутеры 2 (fasthttp)

- [fasthttp](week_05/fasthttp.go)

`fasthttp` быстрый веб-сервер для Go. Уж если задумались о быстрых роутерах, почему бы не оптимизировать веб-сервер.
Использовать с `fasthttprouter`. Совсем другая сигнатура хендлеров.
Мутабельные структуры, вывод происходит после установки всех параметров. С горутинами будет сложно, ибо мутабельность.

### Валидация

- [validation](week_05/validation.go)

Нет стандартного способа валидации, всё проекто-зависимо.
Есть пакеты `asaskevich/govalidator`, `gorilla/schema`, другие.
Это работает на рефлексии. Если надо, чтобы валидация работала быстрее, делайте кодогенератор или пишите руками.

### Фреймворки 1 (Beego)

https://beego.me/ фреймворк

`bee new web_bp; cd web_bp; bee run`
`bee api user; cd user; bee run -gendoc=true -downdoc=true`

Можно сгенерировать сайт, можно сгенерировать API c документацией Swagger.
Много компонент, для быстрого старта очень неплохо.
Ближе к Django.

### Фреймворки 2 (gin)

- [main](week_05/main.go)

`gin-gonic/gin` фреймворк.
Быстрый и функциональный, простой. Генераторов (как в Beego) нет.
Ближе к Flask.

### Логирование

- [logging_main](week_05/logging_main.go)

Несколько подходов к логированию.
Стандартный логгер `log`, `zap` - структурированные логи, `logrus` - делает вид, что структурированный.

`logrus` имеет много коннекторов, для отсылки логов в разные приемники. Медленный.
`zap` на порядок быстрее.

### Веб-сокеты

- [websockets_main](week_05/websockets_main.go) [websockets_index.html](week_05/websockets_index.html)

`gorilla/websockets`, при заходе на нужный урл открывается асинхронный канал на вебсокете.

### Шаблонизация

- [template_main](week_05/template_main.go) [template_base.html](week_05/template_base.html) [template_index.html](week_05/template_index.html)

Стандартный шаблонизатор медленный, на рефлексии.
Шаблонизатор `Hero`, прекомпиляция в код Go.
Программа (шаблон) содержит не текстовые вставки в коде а наоборот, вставки кода в текст.

`go:generate hero -source=./template/` -- комментарий-маркер для команды `go generate`.
Недостаток: шаблон по горячему не перезапустишь. Надо пересобирать программу.

При выводе шаблона используется буфер (выделение/освобождение памяти), надо использовать pool для переиспользования ресурсов.

### Управление зависимостями

- [dependencies_main](week_05/dependencies_main.go)

`go get ...` неудобно, нет автоматики. Нужен менеджер (dep, glide, gb).

`dep init` регистрирует зависимости проекта со всеми версиями.
`vendor` directory с кодом зависимостей.
Все зависимости надо качать себе. Всё равно всё собирается в статический бинарник.

```s
# https://github.com/golang/dep
dep init
dep ensure
dep ensure -add github.com/foo/bar
dep status
dep ensure -update
```

## part 2, week 2 (06)

SQL, NoSQL
[Код, домашки, литература](week_06/materials.zip) https://cloud.mail.ru/public/56Cd/zsGtH4Qo6

### SQL 1 (manual)

- [items.sql](week_06/items.sql) [main.go](week_06/main.go)

Демо простой веб-страницы (без валидаций, обработки ошибок, защиты) с операциями CRUD для сущностей типа "Post" -- записи блога или типа того.

Есть в Го интерфейс `database/sql`,
Этот интерфейс реализован в `go-sql-driver/mysql`, который внутре себя биндится на стандартный интерфейс, в коде используется стандартный `sql`.
Подключение к БД, навешивание хендлеров на роуты, запуск веб-сервера.
Хендлеры -- методы структуры, в которую мы положили коннект к БД.

`sql.NullString` vs `string`: не смог объяснить, чем одно лучше другого.

### SQL 2 (gorm)

- [gorm_main](week_06/gorm_main.go)

А нельзя ли все кишки (типа разбора параметров, генерации запросов, выполнения и проверки результата) завернуть в универсальную оболочку?
Можно. Работать будет либо через рефлексию, либо через кодогенерацию.
Посмотрим на `gorm`: `db, err := gorm.Open("mysql", dsn)`.
Работает через рефлексию, удобно, вполне можно пользоваться для простых вещей.

### SQL 3 (sql injection)

- [sql_injection_main](week_06/sql_injection_main.go)

Используйте плейсхолдеры, не подставляйте параметры руками.

### Key-Value 1 (тегированнй кеш, memcache)

- [memcache](week_06/memcache.go)
- [main](week_06/tcache_main.go) [cache](week_06/tcache_cache.go)

Package `gomemcache/memcache`.

Тегированный кеш: к записям добавлена метаинформация в виде тег:счетчик; счетчик увеличивается при обновлении (метка времени как счетчик),
поэтому можно обоснованно обновлять кеш, зная эти данные.
Разбор (учебного) универсального решения на тегированном кеше.

### Key-Value 2 (redis)

- [main](week_06/redis_main.go) [session](week_06/redis_session.go)

Использование `redigo/redis` для хранения сессий пользователей веб-сайта.
id сессии лежит в куке, данные сессии лежат в redis.
Учебный пример для демонстрации основных операций.

### Прочие системы 1 (Rabbit MQ)

- [form](week_06/rabbit_form.go) [resize_worker](week_06/rabbit_resize_worker.go)

Очереди, Pub/Sub, RabbitMQ `streadway/amqp`.
Демо веб-сервиса создания превьюшек для картинок, в async офлайн-воркере.


### Прочие системы 2 (MongoDB)

- [main](week_06/mongodb_main.go)

Хранилище документов в монго `mgo "gopkg.in/mgo.v2"`, демо.

## part 2, week 3 (07)

Микросервисы
[Код, домашки, литература](week_07/materials.zip) https://cloud.mail.ru/public/GyVL/2YfhX2unF

### Что такое микросервис - 1 (вводная)

Упрощение и борьба со сложностью.
Монолит vs микросервисы.
Монолит: сложно поддерживать, менять, развивать, деплоить, масштабировать.
Микросервисы: накладные расходы на коммуникацию, ошибка в разработке (архитектуре) протоколов приводит к пиздецу.

Первый шаг -- изолировать компоненты "сервиса" за интерфейсами.
Второй шаг -- вынести компоненты в отдельную программу, реализовав коммуникацию через, к примеру, RPC.

### Что такое микросервис - 2 (рефакторинг с целью изоляции)

before
- [lite](week_07/service_lite_before.go)
- [session](week_07/service_session_before.go)

after
before
- [lite](week_07/service_lite_after.go)
- [session](week_07/service_session_after.go)

Демо: веб-страница, логин-пароль, сессия.
Три функции: создать сессию, проверить сессию, удалить сессию.
Используют глобальные переменные: хранилище сессий и мютекс.

Чтобы высадить эту функциональность в изолированный модуль, надо код немного переписать.
Для начала избавиться от глобальных переменных, убрав их в структуру, экземпляр которой будет передаваться как параметр.
Фунции становятся методами этой структуры.

Чтобы иметь возможность играть с реализацией, над структурой-с-методами лепим интерфейс, реализацией которого эта структура и занимается.
Везде, где используются те функции, их вызов меняется на использование интерфейса.

### Делаем микросервис руками - 1 (вынос изолированного модуля в микросервис)

- [server](week_07/het_rpc_server.go)
- [session](week_07/het_rpc_session.go)
- [client_session](week_07/het_rpc_client_session.go)
- [client](week_07/het_rpc_client.go)

Ранее модуль сессии был изолирован. Теперь его можно вынести в микросервис.
Используем `net/rpc`, `net/http` для коммуникации.
Для прода В.Романов не рекомендует его использовать: сериализация чисто гошная, нет "авторизации", ...
Хотя пользовательский интерфейс остался прежним, внутри реализация сессий слегка поменялась:
пришлось переделать сигнатуры методов, дергаемых через RPC, ибо пакет `net/rpc` требует два параметра на входе (вход, буфер_результата) и ошибку на выходе.

В итоге имеем сервер, реализацию логики сессий (используется сервером), клиентскую обёртку на сервисом -- адаптер.
И клиента, которой через адаптер дергает сервер, который выполняет логику.

### Делаем микросервис руками - 2 (jsonrpc server)

- [server](week_07/jsonrpc_server.go)

Сериализация (протокол коммуникации) через JSON, `jsonrpc`.
Гораздо удобнее.
Есть кодек для `net/rpc`.
Пример с сервисом сессий на `jsonrpc`: `err := h.rpcServer.ServeRequest(serverCodec)`.

### protobuf and gRPC - 1 (что такое protobuf)

- [main](week_07/protobuf_main.go)
- [session.proto](week_07/protobuf_session.proto)

Форматы сериализации.
`JSON` -- формат без схемы, тяжело декодировать но легко читать и лепить изменения. Пересылать надо много байт.

`protobuf` -- кодогенерация, жёстко задана схема, экономный, бинарный: (номер поля и тип, длина данных, данные).
По файлу схемы генерится код для любой платформы `protoc --go_out=. *.proto`.
Поля нумеруются, поэтому можно добавлять ранее не задействованные номера => расширяемая схема.
Используется в gRPC.
`golang/protobuf/proto`

### protobuf and gRPC - 2 (grpc framework)

- [session.proto](week_07/grpc_session.proto)
- [session.go](week_07/grpc_session.go)
- [server](week_07/grpc_server.go)
- [lite](week_07/grpc_lite.go)

https://grpc.io/ - фреймворк использующий protobuf. Рекомендован к использованию в прод.

В файле proto появилось описание сервиса `service AuthChecker {...}`.
При кодогенерации нужен плагин: `protoc --go_out=plugins=grpc:. *.proto`.
После генерации мы получаем файл, в котором определены сообщения и интерфейсы для клиента и сервера.
Наряду с кодом, выполняющим коммуникацию.

Для серверной части остается написать реализацию интерфейса. И завернуть это в `grpc.NewServer()` для запуска сервера.

Клиенту надо сделать чуть больше приседаний, чем серверу, чтобы установить соединение.
При вызове grpc функций, первым параметром идёт некий "контекст". В контексте можно управлять разными опциями.

### protobuf and gRPC - 3 (возможности grpc)

- [client](week_07/grpc2_client.go)
- [server](week_07/grpc2_server.go)
- [session](week_07/grpc2_session.go)

Что насчет авторизации, логов, ...?
Есть опции: мидлварь ...Interceptor; метаданные в сообщении ...Credentials, ...
`grpc.WithUnaryInterceptor...grpc.WithPerRPCCredentials...`
Мидлварь позволяет декорировать RPC нужным образом (see aspect-oriented-programming).

Метаданные можно передать при создании соединения, один раз, с использованием `WithPerRPCCredentials`.
Можно передавать в контексте при каждом вызове.
Можно получать метаданные из выполненного вызова `var header, trailer metadata.MD`, через контекст.

На сервере тоже есть возможность использовать декораторы `...Interceptor` и метаданные запроса в контексте.

`grpc.InTapHandle` как инструмент ratelimit. Срабатывает на попытке соединиться, параметры запроса еще не распакованы.

Не надо использовать метаданные как параллельное API. Используйте для накручивания инструментов типа мониторинга, доступа, управления каналом, etc.

### protobuf and gRPC - 4 (streaming)

- [translit.proto](week_07/grpc_stream_translit.proto)
- [translit.go](week_07/grpc_stream_translit.go)
- [client.go](week_07/grpc_stream_client.go)

Особенность grpc -- стриминг. Отправка-прием данных в одном канале, частями.

Посмотрим на придуманный сервис транслитерации ...
У сервиса параметры `... stream Word => stream Word ...`.
Поток слов.

Для отправки и приема сообщений через поток, используются методы `Send`, `Recv`.

В реализации, входной поток служит одновременно и выходным. Мы как считываем из него слово, так и пишем в него ответ, пока не встретим `io.EOF`.

Никто не мешает читать и писать в разных горутинах, асинхронно. Понадобится очередь, для поддержки порядка и буферизации.

Клиент пишет и читает в двух горутинах, используя цикл для итерации по стриму.

### protobuf and gRPC - 5 (Consul: service discovery, load balancing)

- [server](week_07/grpc_lb_server.go)
- [client_lb](week_07/grpc_lb_client_lb.go)

Микросервисы запускаются и падают, разные ноды, хосты, адреса.
Клиентам надо иметь возможность получать информацию о доступных (активных) сервисах -- discovery.

На примере HashiCorp [Consul](https://www.consul.io/)

У него (автора) в докере крутится локальный консул.
Рассмотрим, как в консуле регистрировать сервис, на примере "сервиса сессий" (см. ранее).

Используется grpc и `consul/api`.
Через пакет `flag` и аргументы командной строки задаются параметры соединений с консул и с нашим сервисом.

После старта нашего сервиса идет подключение к консул и использование API для регистрации.
Через defer сразу цепляется функция выноса сервиса из консул.

Продемонстрировал как: запустив два экземпляра сервиса-сессий, посмотреть в консуле их регистрационные записи.

Демонстрация кода клиента с балансировкой нагрузки.
Клиент получает из консул записи доступных (здоровых) сервисов и раскидывает нагрузку по ним.

В примере не показано, но в проде надо использовать health-check параметры и разные метаданные, для фильтрации "здоровых" сервисов.

Балансировка делается прямо в соединении grpc с нашим сервисом, указанием параметров `grpc.WithBlock(), grpc.WithBalancer(...)`

Хитрость в том, что в соединение передается структура `nameResolver`, которая обновляется по ходу течения времени.
В зависимости от данных в консул, которые мы периодически запрашиваем в отдельной горутине.
Горутина делает polling консула раз в 5 секунд и обновляет резолвер.

В принципе, мы не хотим заниматься вот такой ручной балансировкой.
Мы хотим, чтобы облачный инструментарий сам занимался контролем и раскидыванием нагрузки.
Достаточно того, что мы просто регистрируем наш инстанс сервиса в каталоге.

### Дополнительные темы - 1 (JSON REST API grpc-gateway)

- [session.proto](week_07/gateway_session.proto)
- [gateway](week_07/gateway_gateway.go)

grpc-gateway дает возможность использовать под капотом grpc и protobuf, но выглядеть это будет как RESTful JSON API.
Reverse-proxy.
Как это выглядит на практике.

Спека proto дополнена опциями с указанием REST ендпойнтов.
Кодогенерация создает обвязки go, grpc-gateway, swagger.

Код собственно прокси, использующий grpc-gateway, выступает как клиент с сервису grpc.
Запускает коннект к сервису grpc, запускает HTTP сервер построенный на сгенерированном прокси.

### Дополнительные темы - 2 (swagger)

- [session.swagger.json](week_07/swagger_session.swagger.json) сгенерирован сваггером по proto
- [consumer](week_07/swagger_consumer.go)

Open API Specification (OAS) tools, framework.

Декларация API, кодогенерация, документация, тестирование, ...

JSON файл описания API. `swagger serve ...` для запуска интерактива.

`swagger generate client ...`

Демонстрация использования сгенерированного клиентского кода.
Клиент (REST) ходит в grpc-gateway, который обращается к backend на grpc.

Рекомендовано к использованию в проектах с большим количеством сервисов и API.

## part 2, week 4 (08)

https://study.vk.team/learning/play/672
https://cloud.mail.ru/public/ksKr/X5b6hiyA1
