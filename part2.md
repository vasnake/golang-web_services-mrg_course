# Разработка веб-сервисов на Go. Часть 2

[Go course, MRG, Романов Василий](README.md)
[Разработка веб-сервисов на Go: Часть 2](https://study.vk.team/learning/course/672)

- week 1, Анатомия веб-сервиса
- week 2, SQL NoSQL
- week 3, Микросервисы
- week 4, Сервис в работе

## part 2, week 1 (05)

Анатомия веб-сервиса
[Код, домашки, литература](week_05/materials.zip) https://cloud.mail.ru/public/bjWY/TfQTzVMer

### Приветствие 1

Привет, будем галопом по веб-сервисам. Обзорно. Какие задачи встречаются при разработке веб-сервисов.

### Приветствие 2

Анатомия, компоненты веб-сервиса, которые так-или-иначе существуют в разных фреймворках:
- роутинг: отображение урлов (запросов) на функции
- авторизация/аутентификация/идентификация
- логирование
- обработка ошибок
- валидация входный параметров
- шаблонизация
- конфигурировние
- мониторинг
- миддлваре/фреймворки
- работа с IO device (БД, хранилища, очереди, etc.)

### Middleware 1 (wrappers)

- [middleware](week_05/middleware.go)

- Как мог бы выглядеть код миддлваре, если бы мы всё писали руками. Обработка паники, запись в лог, проверка авторизации, бизнес-логика.
Другой вариант: заворачивание хендлеров в "декораторы" middleware.
Middleware в данном случае это фунция handler => handler. Или ServerMux => ServerMux.
Аспект-ориентированое программирование.

### Middleware 2 (metrics using req.context)

- [context_value](week_05/context_value.go)

Используем Context (запроса) для реализации мидлвари, в которой вычисляются тайминги. Ценный пример мониторинга времени, уходящего на обработку запроса.
`req.Context()` как хранилище данных. Каждый вызов бизнес-логики завернут в декоратор, который, с использованием контекста,
вычисляет время, затраченное операцией. Потом миддлварь отсылает собранные в контексте данные в мониторинг, лог.
Перед выполением запроса контекст инициализируется, создается структура в которую будут собираться данные.
На выход навешивается defer, в котором подбиваются итоги. И вызывается бизнес-логика, в которой контекст будет обновляться при сборе данных мониторинга.

Значения в контексте не типизированные, компайл-тайм проверок нет. Поэтому -- очень осторожно и только если по другому нельзя!

Пустой интерфейс -- зло, реализовывать протоколы через них не надо, проверять и отлаживать очень сложно.
Как вариант -- сделайте свой контекст, с конкретными типами.

### Middleware 3 (errors)

- [basic_err](week_05/basic_err.go)
- [named_err](week_05/named_err.go)
- [own_err](week_05/own_err.go)
- [pkg_err](week_05/pkg_err.go)

Как лучше обрабатывать ошибки (в хендлерах запросов):
собрать и вывести информации побольше; трансформировать ошибку в ошибку с подробностями.

Еще лучше: типизированная ошибка. С помощью `errors.New()` создаются константы-ошибки для использования в коде.

Или создается новый тип данных, структура с расширенным описанием ошибки, для использования в качестве возвращаемого значения.
Достаточно реализовать интерфейс `Error() string` и можно пользоваться структурой как стандартной ошибкой.

Универсальный враппер ошибок сделан в `pkg/errors`. `errors.Wrap(...)`, `errors.Cause(err).(type)`, ...
А еще там есть вывод stacktrace.

### Роутеры 1 (gorilla, httprouter)

- [gorilla](week_05/gorilla.go)
- [httprouter](week_05/httprouter.go)
- [multiple](week_05/multiple.go)

`gorilla/mux`, мощный роутер, позволяет задать метод HTTP (POST, GET, ...), задать сложные правила матчинга не только по урл но и по другим параметрам запроса,
многое другое.
Один из самых медленных роутеров.

`HttpRouter`, вполне годная производительность (prefix-tree). Не такой мощный как горилла, сигнатура хендлеров не совместима с оригинальными гошными хендлерами.

Можно испльзовать разные роутеры в разных частях сервиса. Пример с использованием трех роутеров в одном сервисе.
Tradeoff: speed/power

### Роутеры 2 (fasthttp)

- [fasthttp](week_05/fasthttp.go)

`fasthttp` быстрый веб-сервер для Go. Уж если задумались о быстрых роутерах, почему бы не оптимизировать веб-сервер.
Использовать с `fasthttprouter`. Совсем другая сигнатура хендлеров.
Мутабельные структуры, вывод происходит после установки всех параметров. С горутинами будет сложно, ибо мутабельность.

### Валидация

- [validation](week_05/validation.go)

Нет стандартного способа валидации, всё проекто-зависимо.
Есть пакеты `asaskevich/govalidator`, `gorilla/schema`, другие.
Это работает на рефлексии. Если надо, чтобы валидация работала быстрее, делайте кодогенератор или пишите руками.

### Фреймворки 1 (Beego)

https://beego.me/ фреймворк

`bee new web_bp; cd web_bp; bee run`
`bee api user; cd user; bee run -gendoc=true -downdoc=true`

Можно сгенерировать сайт, можно сгенерировать API c документацией Swagger.
Много компонент, для быстрого старта очень неплохо.
Ближе к Django.

### Фреймворки 2 (gin)

- [main](week_05/main.go)

`gin-gonic/gin` фреймворк.
Быстрый и функциональный, простой. Генераторов (как в Beego) нет.
Ближе к Flask.

### Логирование

- [logging_main](week_05/logging_main.go)

Несколько подходов к логированию.
Стандартный логгер `log`, `zap` - структурированные логи, `logrus` - делает вид, что структурированный.

`logrus` имеет много коннекторов, для отсылки логов в разные приемники. Медленный.
`zap` на порядок быстрее.

### Веб-сокеты

- [websockets_main](week_05/websockets_main.go) [websockets_index.html](week_05/websockets_index.html)

`gorilla/websockets`, при заходе на нужный урл открывается асинхронный канал на вебсокете.

### Шаблонизация

- [template_main](week_05/template_main.go) [template_base.html](week_05/template_base.html) [template_index.html](week_05/template_index.html)

Стандартный шаблонизатор медленный, на рефлексии.
Шаблонизатор `Hero`, прекомпиляция в код Go.
Программа (шаблон) содержит не текстовые вставки в коде а наоборот, вставки кода в текст.

`go:generate hero -source=./template/` -- комментарий-маркер для команды `go generate`.
Недостаток: шаблон по горячему не перезапустишь. Надо пересобирать программу.

При выводе шаблона используется буфер (выделение/освобождение памяти), надо использовать pool для переиспользования ресурсов.

### Управление зависимостями

- [dependencies_main](week_05/dependencies_main.go)

`go get ...` неудобно, нет автоматики. Нужен менеджер (dep, glide, gb).

`dep init` регистрирует зависимости проекта со всеми версиями.
`vendor` directory с кодом зависимостей.
Все зависимости надо качать себе. Всё равно всё собирается в статический бинарник.

```s
# https://github.com/golang/dep
dep init
dep ensure
dep ensure -add github.com/foo/bar
dep status
dep ensure -update
```

## part 2, week 2 (06)

SQL, NoSQL
[Код, домашки, литература](week_06/materials.zip) https://cloud.mail.ru/public/56Cd/zsGtH4Qo6

### SQL 1 (manual)

- [items.sql](week_06/items.sql) [main.go](week_06/main.go)

Демо простой веб-страницы (без валидаций, обработки ошибок, защиты) с операциями CRUD для сущностей типа "Post" -- записи блога или типа того.

Есть в Го интерфейс `database/sql`,
Этот интерфейс реализован в `go-sql-driver/mysql`, который внутре себя биндится на стандартный интерфейс, в коде используется стандартный `sql`.
Подключение к БД, навешивание хендлеров на роуты, запуск веб-сервера.
Хендлеры -- методы структуры, в которую мы положили коннект к БД.

`sql.NullString` vs `string`: не смог объяснить, чем одно лучше другого.

### SQL 2 (gorm)

- [gorm_main](week_06/gorm_main.go)

А нельзя ли все кишки (типа разбора параметров, генерации запросов, выполнения и проверки результата) завернуть в универсальную оболочку?
Можно. Работать будет либо через рефлексию, либо через кодогенерацию.
Посмотрим на `gorm`: `db, err := gorm.Open("mysql", dsn)`.
Работает через рефлексию, удобно, вполне можно пользоваться для простых вещей.

### SQL 3 (sql injection)

- [sql_injection_main](week_06/sql_injection_main.go)

Используйте плейсхолдеры, не подставляйте параметры руками.

### Key-Value 1 (тегированнй кеш, memcache)

- [memcache](week_06/memcache.go)
- [main](week_06/tcache_main.go) [cache](week_06/tcache_cache.go)

Package `gomemcache/memcache`.

Тегированный кеш: к записям добавлена метаинформация в виде тег:счетчик; счетчик увеличивается при обновлении (метка времени как счетчик),
поэтому можно обоснованно обновлять кеш, зная эти данные.
Разбор (учебного) универсального решения на тегированном кеше.

### Key-Value 2 (redis)

- [main](week_06/redis_main.go) [session](week_06/redis_session.go)

Использование `redigo/redis` для хранения сессий пользователей веб-сайта.
id сессии лежит в куке, данные сессии лежат в redis.
Учебный пример для демонстрации основных операций.

### Прочие системы 1 (Rabbit MQ)

- [form](week_06/rabbit_form.go) [resize_worker](week_06/rabbit_resize_worker.go)

Очереди, Pub/Sub, RabbitMQ `streadway/amqp`.
Демо веб-сервиса создания превьюшек для картинок, в async офлайн-воркере.


### Прочие системы 2 (MongoDB)

- [main](week_06/mongodb_main.go)

Хранилище документов в монго `mgo "gopkg.in/mgo.v2"`, демо.

## part 2, week 3 (07)

Микросервисы
[Код, домашки, литература](week_07/materials.zip) https://cloud.mail.ru/public/GyVL/2YfhX2unF

### Что такое микросервис - 1 (вводная)

Упрощение и борьба со сложностью.
Монолит vs микросервисы.
Монолит: сложно поддерживать, менять, развивать, деплоить, масштабировать.
Микросервисы: накладные расходы на коммуникацию, ошибка в разработке (архитектуре) протоколов приводит к пиздецу.

Первый шаг -- изолировать компоненты "сервиса" за интерфейсами.
Второй шаг -- вынести компоненты в отдельную программу, реализовав коммуникацию через, к примеру, RPC.

### Что такое микросервис - 2 (рефакторинг с целью изоляции)

before
- [lite](week_07/service_lite_before.go)
- [session](week_07/service_session_before.go)

after
before
- [lite](week_07/service_lite_after.go)
- [session](week_07/service_session_after.go)

Демо: веб-страница, логин-пароль, сессия.
Три функции: создать сессию, проверить сессию, удалить сессию.
Используют глобальные переменные: хранилище сессий и мютекс.

Чтобы высадить эту функциональность в изолированный модуль, надо код немного переписать.
Для начала избавиться от глобальных переменных, убрав их в структуру, экземпляр которой будет передаваться как параметр.
Фунции становятся методами этой структуры.

Чтобы иметь возможность играть с реализацией, над структурой-с-методами лепим интерфейс, реализацией которого эта структура и занимается.
Везде, где используются те функции, их вызов меняется на использование интерфейса.

### Делаем микросервис руками - 1 (вынос изолированного модуля в микросервис)

- [server](week_07/het_rpc_server.go)
- [session](week_07/het_rpc_session.go)
- [client_session](week_07/het_rpc_client_session.go)
- [client](week_07/het_rpc_client.go)

Ранее модуль сессии был изолирован. Теперь его можно вынести в микросервис.
Используем `net/rpc`, `net/http` для коммуникации.
Для прода В.Романов не рекомендует его использовать: сериализация чисто гошная, нет "авторизации", ...
Хотя пользовательский интерфейс остался прежним, внутри реализация сессий слегка поменялась:
пришлось переделать сигнатуры методов, дергаемых через RPC, ибо пакет `net/rpc` требует два параметра на входе (вход, буфер_результата) и ошибку на выходе.

В итоге имеем сервер, реализацию логики сессий (используется сервером), клиентскую обёртку на сервисом -- адаптер.
И клиента, которой через адаптер дергает сервер, который выполняет логику.

### Делаем микросервис руками - 2 (jsonrpc server)

- [server](week_07/jsonrpc_server.go)

Сериализация (протокол коммуникации) через JSON, `jsonrpc`.
Гораздо удобнее.
Есть кодек для `net/rpc`.
Пример с сервисом сессий на `jsonrpc`: `err := h.rpcServer.ServeRequest(serverCodec)`.

### protobuf and gRPC - 1 (что такое protobuf)

- [main](week_07/protobuf_main.go)
- [session.proto](week_07/protobuf_session.proto)

Форматы сериализации.
`JSON` -- формат без схемы, тяжело декодировать но легко читать и лепить изменения. Пересылать надо много байт.

`protobuf` -- кодогенерация, жёстко задана схема, экономный, бинарный: (номер поля и тип, длина данных, данные).
По файлу схемы генерится код для любой платформы `protoc --go_out=. *.proto`.
Поля нумеруются, поэтому можно добавлять ранее не задействованные номера => расширяемая схема.
Используется в gRPC.
`golang/protobuf/proto`

### protobuf and gRPC - 2 (grpc framework)

- [session.proto](week_07/grpc_session.proto)
- [session.go](week_07/grpc_session.go)
- [server](week_07/grpc_server.go)
- [lite](week_07/grpc_lite.go)

https://grpc.io/ - фреймворк использующий protobuf. Рекомендован к использованию в прод.

В файле proto появилось описание сервиса `service AuthChecker {...}`.
При кодогенерации нужен плагин: `protoc --go_out=plugins=grpc:. *.proto`.
После генерации мы получаем файл, в котором определены сообщения и интерфейсы для клиента и сервера.
Наряду с кодом, выполняющим коммуникацию.

Для серверной части остается написать реализацию интерфейса. И завернуть это в `grpc.NewServer()` для запуска сервера.

Клиенту надо сделать чуть больше приседаний, чем серверу, чтобы установить соединение.
При вызове grpc функций, первым параметром идёт некий "контекст". В контексте можно управлять разными опциями.

### protobuf and gRPC - 3 (возможности grpc)

- [client](week_07/grpc2_client.go)
- [server](week_07/grpc2_server.go)
- [session](week_07/grpc2_session.go)

Что насчет авторизации, логов, ...?
Есть опции: мидлварь ...Interceptor; метаданные в сообщении ...Credentials, ...
`grpc.WithUnaryInterceptor...grpc.WithPerRPCCredentials...`
Мидлварь позволяет декорировать RPC нужным образом (see aspect-oriented-programming).

Метаданные можно передать при создании соединения, один раз, с использованием `WithPerRPCCredentials`.
Можно передавать в контексте при каждом вызове.
Можно получать метаданные из выполненного вызова `var header, trailer metadata.MD`, через контекст.

На сервере тоже есть возможность использовать декораторы `...Interceptor` и метаданные запроса в контексте.

`grpc.InTapHandle` как инструмент ratelimit. Срабатывает на попытке соединиться, параметры запроса еще не распакованы.

Не надо использовать метаданные как параллельное API. Используйте для накручивания инструментов типа мониторинга, доступа, управления каналом, etc.

### protobuf and gRPC - 4 (streaming)

- [translit.proto](week_07/grpc_stream_translit.proto)
- [translit.go](week_07/grpc_stream_translit.go)
- [client.go](week_07/grpc_stream_client.go)

Особенность grpc -- стриминг. Отправка-прием данных в одном канале, частями.

Посмотрим на придуманный сервис транслитерации ...
У сервиса параметры `... stream Word => stream Word ...`.
Поток слов.

Для отправки и приема сообщений через поток, используются методы `Send`, `Recv`.

В реализации, входной поток служит одновременно и выходным. Мы как считываем из него слово, так и пишем в него ответ, пока не встретим `io.EOF`.

Никто не мешает читать и писать в разных горутинах, асинхронно. Понадобится очередь, для поддержки порядка и буферизации.

Клиент пишет и читает в двух горутинах, используя цикл для итерации по стриму.

### protobuf and gRPC - 5 (Consul: service discovery, load balancing)

- [server](week_07/grpc_lb_server.go)
- [client_lb](week_07/grpc_lb_client_lb.go)

Микросервисы запускаются и падают, разные ноды, хосты, адреса.
Клиентам надо иметь возможность получать информацию о доступных (активных) сервисах -- discovery.

На примере HashiCorp [Consul](https://www.consul.io/)

У него (автора) в докере крутится локальный консул.
Рассмотрим, как в консуле регистрировать сервис, на примере "сервиса сессий" (см. ранее).

Используется grpc и `consul/api`.
Через пакет `flag` и аргументы командной строки задаются параметры соединений с консул и с нашим сервисом.

После старта нашего сервиса идет подключение к консул и использование API для регистрации.
Через defer сразу цепляется функция выноса сервиса из консул.

Продемонстрировал как: запустив два экземпляра сервиса-сессий, посмотреть в консуле их регистрационные записи.

Демонстрация кода клиента с балансировкой нагрузки.
Клиент получает из консул записи доступных (здоровых) сервисов и раскидывает нагрузку по ним.

В примере не показано, но в проде надо использовать health-check параметры и разные метаданные, для фильтрации "здоровых" сервисов.

Балансировка делается прямо в соединении grpc с нашим сервисом, указанием параметров `grpc.WithBlock(), grpc.WithBalancer(...)`

Хитрость в том, что в соединение передается структура `nameResolver`, которая обновляется по ходу течения времени.
В зависимости от данных в консул, которые мы периодически запрашиваем в отдельной горутине.
Горутина делает polling консула раз в 5 секунд и обновляет резолвер.

В принципе, мы не хотим заниматься вот такой ручной балансировкой.
Мы хотим, чтобы облачный инструментарий сам занимался контролем и раскидыванием нагрузки.
Достаточно того, что мы просто регистрируем наш инстанс сервиса в каталоге.

### Дополнительные темы - 1 (JSON REST API grpc-gateway)

- [session.proto](week_07/gateway_session.proto)
- [gateway](week_07/gateway_gateway.go)

grpc-gateway дает возможность использовать под капотом grpc и protobuf, но выглядеть это будет как RESTful JSON API.
Reverse-proxy.
Как это выглядит на практике.

Спека proto дополнена опциями с указанием REST ендпойнтов.
Кодогенерация создает обвязки go, grpc-gateway, swagger.

Код собственно прокси, использующий grpc-gateway, выступает как клиент с сервису grpc.
Запускает коннект к сервису grpc, запускает HTTP сервер построенный на сгенерированном прокси.

### Дополнительные темы - 2 (swagger)

- [session.swagger.json](week_07/swagger_session.swagger.json) сгенерирован сваггером по proto
- [consumer](week_07/swagger_consumer.go)

Open API Specification (OAS) tools, framework.

Декларация API, кодогенерация, документация, тестирование, ...

JSON файл описания API. `swagger serve ...` для запуска интерактива.

`swagger generate client ...`

Демонстрация использования сгенерированного клиентского кода.
Клиент (REST) ходит в grpc-gateway, который обращается к backend на grpc.

Рекомендовано к использованию в проектах с большим количеством сервисов и API.

## part 2, week 4 (08)

Сервис в работе
[Код, домашки, литература](week_08/materials.zip) https://cloud.mail.ru/public/ksKr/X5b6hiyA1

### Конфигурирование сервиса - 1 (commandline args, cfg file, compile-time)

- [flag](week_08/flag.go)
- [json_config](week_08/json_config.go)
- [ldflags](week_08/ldflags.go)

Пакет `flag` для конфигурирования. Парсинг аргументов командной строки в глобальные переменные.
Можно использовать свой тип данных, для этого реализовать интерфейсный метод `func (x *MyType) Set(input string) error {...}`
и зацепить переменную за пакет `flag` в функции `init`.
До вызова `flag.Parse()` (внутри `main`) в переменных лежат значения по умолчанию.

Другой вариант: создать структуру Config и распаковывать в неё конфиг-файл (json, toml, ...).
Не рекомендует для проектов не помещающихся в один файл. Причина: расползается зависимость от конфига по проекту.
Другой недостаток: нет значений по умолчанию.
Недостатки исправляются заворачиванием конфига в интерфейс.

Можно при сборке бинарника указать через флаги набор значений для внутренних переменных.
Версия программы, бранч GIT, whatever.
`go run -ldflags="-X 'main.Version=$(git rev-parse HEAD)'" ldflags.go`

Вопросы
- Зачем нужна `func init() {...}` в файле? Выполняется до `main`, подробности?
- Как делается dependency injection в Go? Статически, в рантайм?

### Конфигурирование сервиса - 2 (hot-reload config, consul)

- [consul_config](week_08/consul_config.go)

Как в программе освежить конфиг без перезапуска?
hot-reload config, online-conf.
key-value хранилище, как данные конфига, в Consul.
В отдельной горутине крутится логика перечитывания конфига из хранилища.
Конфиг перечитывается только если обновился счетчик `WaitIndex`.

Проблема обновления конфига в середине некоего запроса (бизнес-логика), в котором требуется некая согласованность опций.
На время отработки запроса конфиг менять нельзя?
Для решения проблемы сделан декоратор (мидлварь) запроса, в котором создается локальная копия глобальной мапки конфига.
Эта копия через контекст прокидывается по цепочке обработки.
Мапка это ссылочный тип, мы скопировали ссылку (создали ref) и GC это понимает.
Когда мы обновляем конфиг, мы пересоздаем глобальную мапку, старая ссылка убирается GC как неактуальная, но только после того, как
локальная копия перестанет существовать.
Ещё раз: при обновлении конфига не надо писать данные в существующую мапку! Надо пересоздать мапку!

Как трудно людям жить с мутабельными данными.

N.B. Любое конфигурирование на лету должно рассматриваться в контексте бизнес-логики использования опций конфига.
Необходимо следить за согласованностью, изооляцией и атомарностью (atomicity, consistency, isolation, durability).

### Мониторинг - 1 (expvar, graphite)

- [expvars](week_08/expvars.go)
- [graphite](week_08/graphite.go)

Какие инструменты есть в Go для сбора базовых метрик программы (память, горутины).

Пакет `expvar`. Позволяет зарегать дебаг обработчик в handler веб-страницы.
После чего по урл `/debug/vars` нам доступны некоторые показания.
Мы можем в них добавить свою стату: через создание мапки в пакете или регистрации функции в `expvar.Publish`.

Дергать руками такой урл для просмотра статы -- моветон.
Посмотрим, как использовать сервер статы -- graphite.

В асинхроне крутится `sendStat` процедура.
Она коннектится к графиту, запускает канал таймера, тикер и по тикам:
собирает стату по памяти и горутинам, пишет её в буфер и засылает буфер в графит.
Протокол очень простой, три значения через пробел: `имя_метрики значение метрики таймстемп`.

### Мониторинг - 2 (prometheus)

- [status](week_08/prometheus_status.go)
- [prometheus.yml](week_08/prometheus.yml)
- [metrics](week_08/prometheus_metrics.go)

Как работать с фреймворком prometheus.

На свое приложение навешивается handler `/metrics` => `promhttp.Handler()`.
Этот хендлер выдаёт большую простыню разных метрик, в формате нужном прометею.

Теперь надо настроить прометей, чтобы он ходил на эту страничку.
У прометея есть свой дашборд, но лучше использовать графану.

Ладно, сам сервер (процесс) работает. Как собрать метрики по бизнес-логике?
Используем структуры пакета `prometheus`: `NewSummaryVec`, `NewCounterVec` для сбора количества вызовов своих процедур и времени, затраченного в них.
Заворачиваем процедуры в мидлварь (декораторы), где обновляем метрики.

### Низкоуровневое программирование - 1 (unsafe)

- [unsafe](week_08/unsafe.go)
- [bytetostr](week_08/bytetostr.go)

В.Романов надеется, что этим нам заниматься не придётся.

Пакет `unsafe` -- инструмент компилятора. Уходим из уровня абстракции языка на уровень машины.
Не гарантируется совместимость между версиями Go.
На некоторых облаках запрещено использование unsafe.

Примеры:
Получение адреса в памяти для переменной, размера в байтах.
Преобразование типа 64бит из float64 в int64. Меняем тип у 8 байт. Интерпретация бит в области памяти.

Узнать для полей структуры:
размер, выравнивание, смещение.
Простым изменением порядка полей в структуре мы меняем количество памяти, в которую пакуются поля.
Два булевых флага пакуются в одно слово (8 байт), если они идут подряд. Мы можем в одно слово упаковать до 8 булевых флагов.

Демонстрация устройства слайсов, как оно под капотом.
Преобразование слайса байт в строку без копирования памяти, только созданием нового заголовка.
Строка отличается отсутствием поля capacity в заголовке.

### Низкоуровневое программирование - 2 (c from go)

- [cgo_basic](week_08/cgo_basic.go)
- [main.go](week_08/cgo_main.go)
- [main.c](week_08/cgo_main.c)
- [overhead_test](week_08/cgo_overhead_test.go) [overhead](week_08/cgo_overhead.go)
- [memleak](week_08/cgo_memleak.go)
- [go_sleep](week_08/cgo_go_sleep.go) [cgo_sleep](week_08/cgo_c_sleep.go)

Интеграция кода на Go с кодом на C, вызов сишного кода из Го, cgo.

package `C`.
Это псевдопакет, весь код над строкой с `import "C"` будет скомпилирован GCC (теряется кросс-платформенность/компиляция).
Аргументы и результат вызова сишной функции: надо конвертировать из/в Го.

Учтите, используя через такую связку внешние динамически линкуемые библиотеки, вы теряете преимущества статического бинарника, собираемого Го.

Обратная ситуация, как вызвать Го-шный код из С-шного кода.
Экспортируем Го-шную функцию, используем ее в С-шном коде.
Чтобы избежать двойного объявления С-шных функций, в Го-шном файле мы только декларируем С-шную фунцию, реализация этой функции будет
во внешнем С-шном файле.
Сборка через `go build`.

В процессе вызова фунций из разных рантаймов происходит переключение между рантаймами, оверхед при этом будет какой?
100 наносек на переключение.

В С-шном коде нет GC, надо самому следить за памятью. Например, когда прокидываем строки.

Заметим, в cgo нет асинхронщины "из коробки", все синхронно, в текущем потоке.
Демонстрация того, как Го-шные горутины 100 штук крутятся в 6 потоках.
При этом, С-шные горутины 100 штук потребовали 100 потоков. Ибо С-шный код лочит поток на себя.

Выводы: не надо использовать cgo для большого количества мелких вызовов, хождения по сети.

### Инструменты для статического анализа (go vet, gometalinter)

- [vet_example](week_08/vet_example.go)
- [metalinter_example](week_08/metalinter_example.go)

Какие есть инструменты для поддержания кода в корректном виде, соответствующем стандартам.

`go vet vet_example.go`
`gometalinter metalinter_example.go`

И посмотри на вываливающиеся сообщения.
Полезно прикрутить к CI/CD процессам и не позволять коммитить без проверок.
